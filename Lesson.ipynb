{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXe_7SW6nzLB"
      },
      "source": [
        "# Statistical Modeling in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "EjECFP1RnzLD"
      },
      "source": [
        "### Customization vs Rapid Development\n",
        "\n",
        "As we know from (painful?) experience, Python is powerful because of its ability to leverage `numpy` and `scipy` to implement any statistical model from scratch. We can write the requisite matrix algebra, or the relevant likelihood function, and from there can optimize our model, calculate confidence intervals, and report the output of that model through data frames, lists, or printed tables. Building our own models is great! We get to build a model based on the exact context and assumptions of our problem, and therefore get exactly the model that we wanted. Unfortunately, it takes a LOT of time!\n",
        "\n",
        "This lesson will provide our first exposure to pre-written statistical modeling in Python. We will be able to use only a couple of lines of code to implement complex and valuable statistical and machine learning models. Because the most costly asset in programming is the time that we spend debugging and writing code (running code is MUCH faster and cheaper than the time spent writing code), we are always looking for ways to avoid writing code that someone else has already written.\n",
        "\n",
        "`statsmodels` is a library that covers the majority of regression models commonly used by economists and statisticians in other fields.\n",
        "\n",
        "`sklearn` is an analogous library that covers machine learning models (aside from deep neural networks, which have their own implementations).\n",
        "\n",
        "Each of these libraries is highly optimized to provide performant implementations of models that we use regularly, and allow us to avoid writing these models from scratch unless we need to customize our model for some specific use case! This is great news! You'll never have to think about writing your own linear or logistic regression from scratch again!\n",
        "\n",
        "Let's dive in.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVtUiiGknzLE"
      },
      "source": [
        "## Statsmodels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcc6LHE6nzLE"
      },
      "source": [
        "`statsmodels` makes statistics in Python easy! The library contains tools for regressions ranging from linear regression, to logistic regression, count regressions (negative binomial and poisson), various options for robust covariance measures, and tools to implement time series models as well! There are also really useful tools for assisting in creating our regression model based on any structure that best suits us.\n",
        "\n",
        "We can import `statsmodels` in one of two ways:\n",
        "\n",
        "1) With support for R-style formulas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55BHHFjrnzLE"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ1JPIF5nzLF"
      },
      "source": [
        "    /opt/conda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
        "      import pandas.util.testing as tm\n",
        "\n",
        "\n",
        "This is probably the best way to import our data if we are doing regression analysis for causal inference. In these cases, we are not typically trying to make predictions as new data arrives, and so we do not need to have tools ready to analyze new data using our existing regression models.\n",
        "\n",
        "2) Import `statsmodels` to use pre-built numpy arrays as inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzLY9pp1nzLF"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUohJ0ZNnzLF"
      },
      "source": [
        "In this case, we have other tools that we can use, but we need to manually arrange our `x` and `y` matrices. It looks clunky at first, but can be useful when we are building predictive pipelines using regression models, or when we might want to use both `statsmodels` and `sklearn` with the same data source.\n",
        "\n",
        "Let's start with option 1..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsMmXbA8nzLF"
      },
      "source": [
        "### Preparing a Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAYrc-PgnzLG"
      },
      "source": [
        "When using formulas, we prepare our dataset by importing the data into a Pandas `DataFrame`. We should take care that each of our variables has a name with\n",
        "1) **No spaces**\n",
        "2) No symbols\n",
        "3) Made up of letters and numbers (also can't have a number as the first character)\n",
        "\n",
        "Our code so far might look something like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7q1dkIVnzLG"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdgkTR8UnzLG"
      },
      "source": [
        "Assuming that our data set has already been cleaned. If our data has not yet been cleaned, then we need to clean our data prior to working with either `statsmodels` or `sklearn`. This is because regression AND machine learning models require that all information be provided in numeric format. We need to transform text-based data into categorical data (using either ordered numeric columns or binary variable columns generated from our categories), and ensure that all data is represented in the way that we want to use it within our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3CW0OUWnzLG"
      },
      "source": [
        "### Regression Equations\n",
        "\n",
        "`statsmodels` incorporates `R`-style regression equations by using the `patsy` library behind the scenes. We will talk more about `patsy` soon. The pattern for regression equations is as follows:\n",
        "\n",
        "```\"dependent variable ~ independent variable + another independent variable + any other independent variables\"```\n",
        "\n",
        "The regression equation will be stored in a string (unlike in `R`), and we put our dependent variable (also called the endogenous variable, or outcome of interest) in the leftmost position within the string. We separate the dependent variable from all independent (exogenous or explanatory) variables using the `~` symbol. Then, each independent variable is separated from the others using `+` operators.\n",
        "\n",
        "The reason is is so important that our column names be properly cleaned before implementing regression analysis is that spaces and other problematic formats for column names will cause problems with our regression equations.\n",
        "\n",
        "### Implementing a Model\n",
        "\n",
        "The first model we might try is a simple linear regression. These are the most common regression models, and typically what someone is referring to when they discuss \"running a regression\". The code is wonderfully simple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lz18o3vnzLG"
      },
      "outputs": [],
      "source": [
        "reg = smf.ols(\"hhincome ~ year\", data=data).fit()\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PszUojsznzLG"
      },
      "source": [
        "                                OLS Regression Results                            \n",
        "    ==============================================================================\n",
        "    Dep. Variable:               hhincome   R-squared:                      -0.000\n",
        "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
        "    Method:                 Least Squares   F-statistic:                      -inf\n",
        "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
        "    Time:                        15:24:28   Log-Likelihood:            -1.7131e+05\n",
        "    No. Observations:               13712   AIC:                         3.426e+05\n",
        "    Df Residuals:                   13711   BIC:                         3.426e+05\n",
        "    Df Model:                           0                                         \n",
        "    Covariance Type:            nonrobust                                         \n",
        "    ==============================================================================\n",
        "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
        "    ------------------------------------------------------------------------------\n",
        "    Intercept      0.0188      0.000    138.414      0.000       0.019       0.019\n",
        "    year          37.8564      0.274    138.414      0.000      37.320      38.392\n",
        "    ==============================================================================\n",
        "    Omnibus:                     9819.620   Durbin-Watson:                   1.027\n",
        "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):           250725.793\n",
        "    Skew:                           3.151   Prob(JB):                         0.00\n",
        "    Kurtosis:                      22.978   Cond. No.                     9.31e+17\n",
        "    ==============================================================================\n",
        "    \n",
        "    Warnings:\n",
        "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
        "    [2] The smallest eigenvalue is 6.41e-26. This might indicate that there are\n",
        "    strong multicollinearity problems or that the design matrix is singular.\n",
        "\n",
        "\n",
        "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
        "      return self.ess/self.df_model\n",
        "\n",
        "\n",
        "When we run these two lines of code, we are creating, fitting, and reporting on a regression model! It's fast, it's clean, and it's really easy to implement! `sm.ols` is the OLS class of regression models, and takes two required arguments: a regression equation (passed as a string), and a data source (expected to be a `pandas.DataFrame` object). We use the `.fit()` method to complete all of the math that actually solves our regression model. When we call `.summary()` on a fitted regression, we get a printout of the regression summary tables for the model, complete with diagnostic measures, estimates of our beta coefficients, and confidence intervals!\n",
        "\n",
        "If the model is satisfactory, then we are done! (It really is that simple!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDIZPVsSnzLH"
      },
      "source": [
        "If I want to keep iterating on my model, I might want to try regressing year on the logged average household incomes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJbRKEncnzLH"
      },
      "outputs": [],
      "source": [
        "reg = smf.ols(\"np.log(hhincome) ~ year\", data=data[data['hhincome']>0]).fit()\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnr8vqNZnzLH"
      },
      "source": [
        "                                OLS Regression Results                            \n",
        "    ==============================================================================\n",
        "    Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
        "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
        "    Method:                 Least Squares   F-statistic:                      -inf\n",
        "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
        "    Time:                        15:31:18   Log-Likelihood:                -17363.\n",
        "    No. Observations:               13653   AIC:                         3.473e+04\n",
        "    Df Residuals:                   13652   BIC:                         3.474e+04\n",
        "    Df Model:                           0                                         \n",
        "    Covariance Type:            nonrobust                                         \n",
        "    ==============================================================================\n",
        "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
        "    ------------------------------------------------------------------------------\n",
        "    Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
        "    year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
        "    ==============================================================================\n",
        "    Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
        "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
        "    Skew:                          -1.469   Prob(JB):                         0.00\n",
        "    Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
        "    ==============================================================================\n",
        "    \n",
        "    Warnings:\n",
        "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
        "    [2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
        "    strong multicollinearity problems or that the design matrix is singular.\n",
        "\n",
        "\n",
        "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
        "      return self.ess/self.df_model\n",
        "\n",
        "\n",
        "As you can see from the code above, everything is the same, except that we were able to transform household income using `np.log` on the go! We don't even need to create a new column! We can just do it inside of our regression model! We also subset our data so that the log operator doesn't break our model by introducing $-\\infty$ as a possible `hhincome` value.\n",
        "\n",
        "In other cases, it might be useful to create state-level fixed effects by including dummy variables for the states in our `statefip` column. Note that this won't work with our current data, since we only have one state in our data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR6oqsIznzLH"
      },
      "outputs": [],
      "source": [
        "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data[data['hhincome']>0]).fit()\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhKg_CkznzLH"
      },
      "source": [
        "                                OLS Regression Results                            \n",
        "    ==============================================================================\n",
        "    Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
        "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
        "    Method:                 Least Squares   F-statistic:                      -inf\n",
        "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
        "    Time:                        15:32:19   Log-Likelihood:                -17363.\n",
        "    No. Observations:               13653   AIC:                         3.473e+04\n",
        "    Df Residuals:                   13652   BIC:                         3.474e+04\n",
        "    Df Model:                           0                                         \n",
        "    Covariance Type:            nonrobust                                         \n",
        "    ==============================================================================\n",
        "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
        "    ------------------------------------------------------------------------------\n",
        "    Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
        "    year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
        "    ==============================================================================\n",
        "    Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
        "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
        "    Skew:                          -1.469   Prob(JB):                         0.00\n",
        "    Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
        "    ==============================================================================\n",
        "    \n",
        "    Warnings:\n",
        "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
        "    [2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
        "    strong multicollinearity problems or that the design matrix is singular.\n",
        "\n",
        "\n",
        "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
        "      return self.ess/self.df_model\n",
        "\n",
        "\n",
        "The `C()` command indicates that we would like to consider the `statefip` variable as a **C**ategorical variable, not a numeric variable. We can transform ANY column using the categorical operator. It is most useful when a column is text-based, or when a column is numeric but should not be treated as a count, ordinal, or continuous variable. We CAN use it on our dependent variable, but this will (unless our dependent variable was binary text data) break our regression model, which expects only a single dependent variable, rather than an array of dependent variables.\n",
        "\n",
        "Sometimes we want to include transformed variables in our model without creating a new column. The `I()` operator allows us to do just that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL5qgzAtnzLH"
      },
      "outputs": [],
      "source": [
        "# Square a variable using the I() function for\n",
        "#   mathematical transformations\n",
        "reg = smf.ols(\"np.log(hhincome) ~ age + I(age**2)\", data=data).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBdntbXInzLI"
      },
      "source": [
        "In this case, we transform `age` by squaring it (maybe in preparation to create an age-earnings profile?). One line, simple syntax, what could be better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfHL7VBxnzLI"
      },
      "outputs": [],
      "source": [
        "# Combine variables using the I() function for\n",
        "#   mathematical transformations\n",
        "reg = smf.ols(\"np.log(hhincome) ~ I(age-education-5)\", data=data).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mMpSHEjnzLI"
      },
      "source": [
        "This example combines TWO columns to create a new measure (proxying experience by subtracting education from age, and subtracting an additional 5 years). All we have to do is describe the relationship that we want to model as an explanatory variable, and we are off to the races! Most operators are fair game, and we can include an arbitrary number of columns in our measure calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py1eoEc9nzLI"
      },
      "source": [
        "### More robust modeling\n",
        "\n",
        "If we want to utilize robust standard errors, we can easily update our regression results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuChIiXXnzLI"
      },
      "outputs": [],
      "source": [
        "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data).fit()\n",
        "# Use White's (1980) Standard Error\n",
        "reg.get_robustcov_results(cov_type='HC0')\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B73x9tHcnzLI"
      },
      "source": [
        "Or, if we want to cluster our standard errors by state,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_Z-A2xfnzLI"
      },
      "outputs": [],
      "source": [
        "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data).fit()\n",
        "# Use Cluster-robust Standard Errors\n",
        "reg.get_robustcov_results(cov_type='cluster', groups=data['statefip']) # Need to specify groups\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeUBC2i9nzLI"
      },
      "source": [
        "We don't have to stick to just `HC0` and cluster-robust standard errors. Below are some of the [covariance options](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.get_robustcov_results.html) that we have:\n",
        "1) `HC0`: White's (1980) Heteroskedasticity robust standard errors\n",
        "2) `HC1`, `HC2`, `HC3`: MacKinnon and White's (1985) alternative robust standard errors, with `HC3` being designed for improved performance in small samples\n",
        "3) `cluster`: Cluster robust standard errors\n",
        "4) `hac-panel`: Panel robust standard errors\n",
        "\n",
        "We should choose the standard errors that best fit our specific data needs, and it is important to realize that this choice is highly context-dependent. The structure and nature of our data should be carefully considered, as should the specific regression model that we are trying to implement.\n",
        "\n",
        "### Time Series Models\n",
        "\n",
        "Not only can we model linear regression, we also have multiple time series options available. We won't go into much detail, since each of these models deserve to have significant time devoted to them, and we just don't have the time in this class.\n",
        "\n",
        "- [ARIMA](http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html) models\n",
        "- [VAR](http://www.statsmodels.org/dev/generated/statsmodels.tsa.vector_ar.var_model.VAR.html) models\n",
        "- [Exponential Smoothing](https://www.statsmodels.org/stable/tsa.html#exponential-smoothing) models\n",
        "\n",
        "We can run an ARIMA, for example, using code like the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjNjSqH4nzLJ"
      },
      "outputs": [],
      "source": [
        "# This won't work unless we have multiple years of data (which we currently don't)\n",
        "\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "\n",
        "y = data.loc[data['statefip']==31, ['hhincome','year']]\n",
        "y.index=pd.to_datetime(y.year)\n",
        "reg = ARIMA(y['hhincome'], order=(1,1,0)).fit()\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r9wMK77nzLJ"
      },
      "source": [
        "### Modeling Discrete Outcomes\n",
        "\n",
        "If we have a [binary dependent variable](https://www.statsmodels.org/devel/discretemod.html), we are able to use either [Logit](https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.Logit.html#statsmodels.discrete.discrete_model.Logit) or [Probit](https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.Probit.html#statsmodels.discrete.discrete_model.Probit) models to estimate the effect of exogenous variables on our outcome of interest. To fit a Logit model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-SEQ_LRnzLJ"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "myformula=\"married ~ hhincome + C(statefip) + C(year) + educ\"\n",
        "model= sm.Logit.from_formula(myformula, data=data).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OySegCIGnzLJ"
      },
      "source": [
        "### Modeling Count Data\n",
        "\n",
        "When modeling count data, we have options such as [Poisson](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Poisson.html#statsmodels.discrete.discrete_model.Poisson) and [Negative Binomial](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.NegativeBinomial.html#statsmodels.discrete.discrete_model.NegativeBinomial) models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfozK8wnzLJ"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/raw/master/DataSets/auto-mpg.csv\")\n",
        "\n",
        "myformula=\"nchild ~ hhincome + C(statefip) + C(year) + educ + married\"\n",
        "\n",
        "model= sm.Poisson.from_formula(myformula, data=data).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PDpNonhnzLJ"
      },
      "source": [
        "There are many other regression \"flavors\", and the best way to learn about what is available through `statsmodels` is to [read the docs](https://www.statsmodels.org/stable/user-guide.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cc1gEppnzLJ"
      },
      "source": [
        "## The `patsy` library\n",
        "\n",
        "We have been using regression equations in `statsmodels` a lot without really discussing what is happening behind the scenes. `statsmodels` relies on a library called `patsy` to parse regression equations and prepare our data for regression analysis. While `statsmodels` does a great job of incorporating the `patsy` library for us, this isn't always the case. In fact, it is a really valuable tool in many other contexts (think machine learning or deep learning).\n",
        "\n",
        "\n",
        "### Why use `patsy`?\n",
        "\n",
        "We don't necessarily have to use `patsy`. We could just select our variables manually. Creating a column of ones to serve as our intercept column is trivial (you of course remember that from the linear regression assignment). `patsy` is a tool for creating a standardized pipeline to deal with data that is stored in identical formats, and aids us in creating reusable or replicable code. Patsy allows us to separate our endogenous and exogenous variables AND to\n",
        "\t- \"Dummy out\" categorical variables\n",
        "\t- Easily transform variables (square, or log transforms, etc.)\n",
        "\t- Use identical transformations on future data\n",
        "    \n",
        "Even better, `patsy` is just as easy to use as regression equations. We just need to learn about the function wrappers that are necessary to create our processed data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB4YReeCnzLK"
      },
      "outputs": [],
      "source": [
        "import patsy as pt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")\n",
        "\n",
        "# To create y AND x matrices\n",
        "y, x = pt.dmatrices(\"hhincome ~ year + educ + married + age\", data = data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssfKTu4nnzLK"
      },
      "source": [
        "In order to get started, we need to import `patsy`, and we typically give it the two-letter abbreviation `pt`. Once we have imported our data, we use the `pt.dmatrices` function. This function takes a regression equation (again, as a string), and a data source. The returned value is a **tuple** of `y` and `x`. We can break that tuple into two values by using the `y, x = ...` syntax, so that we have a `y` array and an `x` array.\n",
        "\n",
        "We don't have to create BOTH `y` and `x` data, though! We can use the `pt.dmatrix` function to just create an `x` matrix. Maybe we already have a dependent variable, and want to try out variations on our explanatory variables to see how each performs. In this case, our regression equation should have no column name to the left of the `~` symbol:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4Njbe4gnzLK"
      },
      "outputs": [],
      "source": [
        "# To create ONLY an x matrix\n",
        "x = pt.dmatrix(\"~ year + educ + married + age\", data = data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ5GtMTenzLO"
      },
      "source": [
        "One more note is that these regression equations automatically include an intercept term. If you do NOT want an intercept term (some regression models and most machine learning models don't use them), then you can add `-1` as an exogenous variable in your regression equation, in order to indicate that you want to eliminate the column of ones that make up the intercept column in our matrix of exogenous regressors.\n",
        "\n",
        "### Categorical Variables\n",
        "\n",
        "Again, we have the functions described in the regression section above available to us as we transform our data. We can create categorical variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZFdWMKXnzLO"
      },
      "outputs": [],
      "source": [
        "# To create y AND x matrices\n",
        "eqn = \"hhincome ~ C(year) + educ + married + age\"\n",
        "y, x = pt.dmatrices(eqn, data = data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jto1imhnzLP"
      },
      "source": [
        "And (again) we can transform variables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXvSwtrJnzLP"
      },
      "outputs": [],
      "source": [
        "# To create y AND x matrices\n",
        "eqn = \"I(np.log(hhincome)) ~ C(year) + educ + married + age + I(age**2)\"\n",
        "y, x = pt.dmatrices(eqn, data = data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiSdMmrJnzLP"
      },
      "source": [
        "We can also use interaction operators. `*` will interact each value of two columns, and also include the original columns in the regression model. `:` will include only the interaction terms, while omitting the original columns. Check out the [explanation of formulas](https://patsy.readthedocs.io/en/latest/formulas.html) for more details.\n",
        "\n",
        "\n",
        "### SUPER IMPORTANT $\\rightarrow$ Same Transformation on New Data!\n",
        "\n",
        "Often, we will want to build a model with observed data that can make predictions about new observations as those observations are recorded. `patsy` provides a simple function to take the structure of one exogenous matrix and generate another identically structured matrix using new data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVq90nWbnzLP"
      },
      "outputs": [],
      "source": [
        "# To create a new x matrix based on our previous version\n",
        "xNew = pt.build_design_matrices([x.design_info], dataNew)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-OWwucFnzLP"
      },
      "source": [
        "In other words, we can create a new matrix in the SAME SHAPE as our original `x` matrix by using the `build_design_matrices()` function in `patsy`.\n",
        "\n",
        "We pass a list containing the old design matrix information (because we can actually create many matrices simultaneously), as well as the new data from which to construct our new matrix.\n",
        "\n",
        "Why does recreating our `x` array matter? This process ensures that we always have the same number of categories in our categorical variables. A new, smaller subset of data that is freshly observed may not contain observations of every category, in which case an updated patsy matrix would not contain the correct number of columns! We are able to maintain consistency in our model, making our work replicable. Most importantly, this will streamline the use of `statsmodels` and `sklearn` in the same workflow!\n",
        "\n",
        "Speaking of `sklearn`..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzGjFFxRnzLP"
      },
      "source": [
        "## `sklearn`\n",
        "\n",
        "What `statsmodels` does for regression analysis, `sklearn` does for predictive analytics and machine learning. It is a truly fabulous library. `sklearn` is likely the most popular machine learning library, and has a standard API to make using the library VERY simple. Even better, it's documentation is some of the nicest documentation you will find anywhere, and contains incredible detail about how to implement models, as well as lessons about the \"how\" and \"why\" of using each model. You couldn't write a better textbook about machine learning than the documentation for `sklearn`.\n",
        "\n",
        "Below, we will briefly discuss some of the models that are most commonly utilized from `sklearn`. Details will be sparse. We are mostly focused on the code implementation of these models. More detail on how machine learning models work is provided in  Business Forecasting, and is outside the scope of this course.\n",
        "\n",
        "### Decision Tree Classification (and Regression)\n",
        "\n",
        "[Classification](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) and [Regression](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) Trees (CARTs) are the standard jumping-off point for exploring machine learning. They are very easy to implement in `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtBcYs2inzLP"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import patsy as pt\n",
        "\n",
        "data = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/roomOccupancy.csv\")\n",
        "\n",
        "y, x = pt.dmatrices(\"Occupancy ~ CO2\", data=data)\n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(x, y.squeeze())\n",
        "\n",
        "pred = clf.predict(x)\n",
        "\n",
        "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FnkDJz3nzLQ"
      },
      "source": [
        "    In-sample accuracy: 0.9753162225224119\n",
        "\n",
        "\n",
        "### Support Vector Machines\n",
        "\n",
        "We also implement [Support Vector Machines](http://scikit-learn.org/stable/modules/svm.html#svm) for both [classification](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) and [regression](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp-v3MNynzLQ"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = svm.SVC()\n",
        "clf = clf.fit(x, y.squeeze())\n",
        "\n",
        "pred = clf.predict(x)\n",
        "\n",
        "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8RMugEFnzLQ"
      },
      "source": [
        "    /opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
        "      \"avoid this warning.\", FutureWarning)\n",
        "\n",
        "\n",
        "    In-sample accuracy: 0.9397028122313643\n",
        "\n",
        "\n",
        "Can you see the API pattern yet?\n",
        "\n",
        "### Random Forest Models\n",
        "\n",
        "Again, available in both [classification](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) and [regression](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) flavors, these models are aggregations of many randomized Decision Trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu8iCNvrnzLQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=50)\n",
        "clf = clf.fit(x, y.squeeze())\n",
        "\n",
        "pred = clf.predict(x)\n",
        "\n",
        "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVgYSgsQnzLQ"
      },
      "source": [
        "    In-sample accuracy: 0.9748250030701215\n",
        "\n",
        "\n",
        "There MUST be a pattern here...\n",
        "\n",
        "Of course there is! We import our classifier (or regressor), then create an instance of that object. We can name it `clf` or anything else that we prefer. From there, the process is the same:\n",
        "- Use the `.fit()` method, passing in the relevant data for our context\n",
        "- Create predictions using our fitted model with `.predict()` and new exogenous data (or the old data to test in-sample fit)\n",
        "- Measure the performance of our model with `accuracy_score`, or any other metric that can describe performance given a specific use case\n",
        "\n",
        "### More from `sklearn`\n",
        "\n",
        "Many other tools are also available to aid in the data cleaning process through `sklearn`. Some of these are:\n",
        "\n",
        "- [Principal Component Analysis (PCA)](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n",
        "- [Factor Analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis)\n",
        "- Many [Cross-Validation Algorithms](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
        "- [Hyperparameter Tuning](http://scikit-learn.org/stable/modules/grid_search.html)\n",
        "   - Finding the correct parameters for a decision tree or random forest, for example\n",
        "- [Model Evaluation Tools](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
        "- [Plotting decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp-HkPZ_nzLQ"
      },
      "source": [
        "## Solve-it!\n",
        "\n",
        "Using the wage data provided here (https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/wagePanelData.csv), create a linear regression model to explain and/or predict wages. Your data set should be labeled `data` and your fitted model should be stored as `reg`. If you do not name the model correctly, you won't get any points!\n",
        "\n",
        "Please put all your code for this exercise in the cell labeled `#si-linear-regression` file found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3M1yNUwmnzLQ",
        "outputId": "e65270bd-2dcb-4ffb-cb71-6078be76a0bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:               log_wage   R-squared:                       0.269\n",
            "Model:                            OLS   Adj. R-squared:                  0.268\n",
            "Method:                 Least Squares   F-statistic:                     509.4\n",
            "Date:                Thu, 05 Dec 2024   Prob (F-statistic):          6.34e-282\n",
            "Time:                        00:21:13   Log-Likelihood:                -2037.4\n",
            "No. Observations:                4165   AIC:                             4083.\n",
            "Df Residuals:                    4161   BIC:                             4108.\n",
            "Df Model:                           3                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "====================================================================================\n",
            "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------------\n",
            "Intercept            5.4900      0.034    160.679      0.000       5.423       5.557\n",
            "education            0.0735      0.002     32.487      0.000       0.069       0.078\n",
            "is_black            -0.2656      0.024    -11.159      0.000      -0.312      -0.219\n",
            "years_experience     0.0131      0.001     22.985      0.000       0.012       0.014\n",
            "==============================================================================\n",
            "Omnibus:                       51.097   Durbin-Watson:                   0.673\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               74.286\n",
            "Skew:                          -0.138   Prob(JB):                     7.40e-17\n",
            "Kurtosis:                       3.594   Cond. No.                         143.\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ],
      "source": [
        "#si-linear-regression\n",
        "\n",
        "#Passed\n",
        "import statsmodels.formula.api as smf\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "data = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/wagePanelData.csv\")\n",
        "\n",
        "data.columns\n",
        "\n",
        "reg = smf.ols(\"log_wage ~ education + is_black + years_experience\", data=data).fit()\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A682Ou03nzLR"
      },
      "source": [
        "## Solve-it!\n",
        "\n",
        "Import the pass/fail data for students in Portugal found here(https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/passFailTrain.csv), and create a logistic regression model\n",
        "using `statsmodels` that can estimate the likelihood of students passing or failing class. The dependent variable is contained in the column called `G3`, which takes the value `1` when the student has a passing final grade, and `0` otherwise.\n",
        "\n",
        "Call your fitted model `reg`, and place all code for this exercise in the cell labeled `#si-logistic-regression` file found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X2I4HqsnzLR",
        "outputId": "9e870bf4-9be3-43db-bf1f-95d75fb0144b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.616319\n",
            "         Iterations 5\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:                     G3   No. Observations:                  296\n",
            "Model:                          Logit   Df Residuals:                      292\n",
            "Method:                           MLE   Df Model:                            3\n",
            "Date:                Thu, 05 Dec 2024   Pseudo R-squ.:                 0.01393\n",
            "Time:                        00:35:20   Log-Likelihood:                -182.43\n",
            "converged:                       True   LL-Null:                       -185.01\n",
            "Covariance Type:            nonrobust   LLR p-value:                    0.1610\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept     -0.2610      0.485     -0.538      0.590      -1.211       0.689\n",
            "studytime      0.3131      0.168      1.862      0.063      -0.017       0.643\n",
            "sex            0.3894      0.273      1.428      0.153      -0.145       0.924\n",
            "internet       0.2606      0.332      0.786      0.432      -0.389       0.911\n",
            "==============================================================================\n"
          ]
        }
      ],
      "source": [
        "#si-logistic-regression\n",
        "\n",
        "#for binary dependent variables use logit!\n",
        "\n",
        "import statsmodels.formula.api as smf\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "data2 = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/passFailTrain.csv\")\n",
        "\n",
        "data2.columns\n",
        "\n",
        "reg = smf.logit(\"G3 ~ studytime + sex + internet\", data=data2).fit()\n",
        "print(reg.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi7dvJJxnzLR"
      },
      "source": [
        "## Solve-it!\n",
        "\n",
        "Use the data on NFL franchise values included in the NFL Valuation data source (https://raw.githubusercontent.com/dustywhite7/Econ8320/master/AssignmentData/assignment12Data.csv) file to implement a Random Forest Classifier in sklearn using 100 trees to predict team-years when `Playoffs` takes the value `1` (when a team made the playoffs in that season).\n",
        "\n",
        "- Use Patsy to create `x2` and `y2` matrices\n",
        "- Create the classifier\n",
        "- Fit the classifier, and store the fitted model with the name `playoffForest`\n",
        "\n",
        "Place all code for this exercise in the cell labeled `#si-random-forest` file found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "08dnkkRlnzLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8294570c-8c01-4c26-d967-78551394bb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 OLS Regression Results                                \n",
            "=======================================================================================\n",
            "Dep. Variable:               Playoffs   R-squared (uncentered):                   0.451\n",
            "Model:                            OLS   Adj. R-squared (uncentered):              0.440\n",
            "Method:                 Least Squares   F-statistic:                              40.27\n",
            "Date:                Thu, 05 Dec 2024   Prob (F-statistic):                    1.72e-13\n",
            "Time:                        01:59:43   Log-Likelihood:                         -64.823\n",
            "No. Observations:                 100   AIC:                                      133.6\n",
            "Df Residuals:                      98   BIC:                                      138.9\n",
            "Df Model:                           2                                                  \n",
            "Covariance Type:            nonrobust                                                  \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "SuperBowl      0.6488      0.197      3.297      0.001       0.258       1.039\n",
            "Year           0.0002    2.4e-05      7.285      0.000       0.000       0.000\n",
            "==============================================================================\n",
            "Omnibus:                      280.454   Durbin-Watson:                   2.087\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               16.555\n",
            "Skew:                           0.644   Prob(JB):                     0.000254\n",
            "Kurtosis:                       1.478   Cond. No.                     8.45e+03\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
            "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[3] The condition number is large, 8.45e+03. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "0.52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#si-random-forest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import patsy as pt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/dustywhite7/Econ8320/master/AssignmentData/assignment12Data.csv\")\n",
        "\n",
        "data\n",
        "\n",
        "#do i need to make the samples?\n",
        "data4 = data.sample(100)\n",
        "data5 = data.sample(100)\n",
        "\n",
        "y, x = pt.dmatrices(\"Playoffs ~ -1 + SuperBowl + Year \", data=data4)\n",
        "\n",
        "model = sm.OLS(y,x).fit()\n",
        "print(model.summary())\n",
        "\n",
        "x2 = pt.build_design_matrices([x.design_info], data5)\n",
        "\n",
        "y2 = pt.build_design_matrices([y.design_info], data5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x1, x2, y1, y2 = train_test_split(x, y)\n",
        "\n",
        "playoffForest_noFit = RandomForestClassifier(n_estimators = 100) #do have to provide one argument for the number of \"trees\" in our \"fores\"\n",
        "playoffForest = playoffForest_noFit.fit(x1, y1)\n",
        "\n",
        "pred = playoffForest.predict(x2)\n",
        "\n",
        "print(accuracy_score(y2, pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing code\n",
        "#y1\n",
        "import re\n",
        "\n",
        "(bool(re.search(r'RandomForestClassifier', str(type(playoffForest)))), \"You didn't make a random forest!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lb3rBjCZJGN",
        "outputId": "67502dfd-7c2e-4014-bba8-01b1fb6a235c"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, \"You didn't make a random forest!\")"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--"
      ],
      "metadata": {
        "id": "asAMjEAooPh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--\n",
        "\n"
      ],
      "metadata": {
        "id": "_AY-GR6_oP6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LECTURE NOTES\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zOIjNqwhoQI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#modeling through Statsmodels/Sklearn\n",
        "\n",
        "#OLS\n",
        "\n",
        "import statsmodels.api as sm  #this is the non formula methode for importing stats models that requires numbpy arrays\n",
        "#this is easier because it allows for ineroperability when switching through regressions and machine learning\n",
        "\n",
        "\n",
        "#startign with this option first\n",
        "import statsmodels.formula.api as smf #easiest way to import stats models and allows us to use regression equatons (like r) in order to run regressions very easily\n",
        "\n",
        "#for data sets\n",
        "#1. No spaces in names, no symbols names, made up of letters and numbers in names (no numbers at the start)"
      ],
      "metadata": {
        "id": "HcMaVclvodJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing a dataset\n",
        "\n",
        "import statsmodels.formula.api as smf\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")"
      ],
      "metadata": {
        "id": "0QOUe2ADqMgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data.columns\n",
        "\n",
        "#model hourly wage a dependent variable ~ then independent variables (JUST LIKE R, yayyyy) [age and educations], then call on the dataset were pulling regression info\n",
        "#from data = data\n",
        "model = smf.ols(\"hrwage ~ age + educ\", data=data)"
      ],
      "metadata": {
        "id": "yFU3pZ0Eqn5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#doe the model fit (.fit) to find out parameters (beta coeficients)\n",
        "modelFit = model.fit()\n",
        "\n",
        "modelFit.summary() #this gives us our stats summary table\n",
        "\n",
        "#from table we can see that as age increases we have a small increase in hourly wage\n",
        "#as education level increases we have a non trivial increase in wages ($2.65 in wage for every education level increase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "LYat3MIMsCEa",
        "outputId": "e5fc9840-ec7a-4725-ab93-5aa175c373ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                 hrwage   R-squared:                       0.075\n",
              "Model:                            OLS   Adj. R-squared:                  0.075\n",
              "Method:                 Least Squares   F-statistic:                     406.1\n",
              "Date:                Wed, 04 Dec 2024   Prob (F-statistic):          2.65e-170\n",
              "Time:                        22:01:34   Log-Likelihood:                -44037.\n",
              "No. Observations:               10008   AIC:                         8.808e+04\n",
              "Df Residuals:                   10005   BIC:                         8.810e+04\n",
              "Df Model:                           2                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept     -7.7081      0.980     -7.867      0.000      -9.629      -5.787\n",
              "age            0.1168      0.013      8.963      0.000       0.091       0.142\n",
              "educ           2.6515      0.097     27.324      0.000       2.461       2.842\n",
              "==============================================================================\n",
              "Omnibus:                    19307.010   Durbin-Watson:                   1.966\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         97351740.228\n",
              "Skew:                          14.707   Prob(JB):                         0.00\n",
              "Kurtosis:                     485.278   Cond. No.                         240.\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>         <td>hrwage</td>      <th>  R-squared:         </th> <td>   0.075</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.075</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   406.1</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Wed, 04 Dec 2024</td> <th>  Prob (F-statistic):</th> <td>2.65e-170</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>22:01:34</td>     <th>  Log-Likelihood:    </th> <td> -44037.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td> 10008</td>      <th>  AIC:               </th> <td>8.808e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td> 10005</td>      <th>  BIC:               </th> <td>8.810e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>   -7.7081</td> <td>    0.980</td> <td>   -7.867</td> <td> 0.000</td> <td>   -9.629</td> <td>   -5.787</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>age</th>       <td>    0.1168</td> <td>    0.013</td> <td>    8.963</td> <td> 0.000</td> <td>    0.091</td> <td>    0.142</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>educ</th>      <td>    2.6515</td> <td>    0.097</td> <td>   27.324</td> <td> 0.000</td> <td>    2.461</td> <td>    2.842</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>19307.010</td> <th>  Durbin-Watson:     </th>   <td>   1.966</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>97351740.228</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td>14.707</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td>485.278</td>  <th>  Cond. No.          </th>   <td>    240.</td>  \n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &      hrwage      & \\textbf{  R-squared:         } &      0.075    \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.075    \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      406.1    \\\\\n\\textbf{Date:}             & Wed, 04 Dec 2024 & \\textbf{  Prob (F-statistic):} &  2.65e-170    \\\\\n\\textbf{Time:}             &     22:01:34     & \\textbf{  Log-Likelihood:    } &    -44037.    \\\\\n\\textbf{No. Observations:} &       10008      & \\textbf{  AIC:               } &  8.808e+04    \\\\\n\\textbf{Df Residuals:}     &       10005      & \\textbf{  BIC:               } &  8.810e+04    \\\\\n\\textbf{Df Model:}         &           2      & \\textbf{                     } &               \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &               \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept} &      -7.7081  &        0.980     &    -7.867  &         0.000        &       -9.629    &       -5.787     \\\\\n\\textbf{age}       &       0.1168  &        0.013     &     8.963  &         0.000        &        0.091    &        0.142     \\\\\n\\textbf{educ}      &       2.6515  &        0.097     &    27.324  &         0.000        &        2.461    &        2.842     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 19307.010 & \\textbf{  Durbin-Watson:     } &      1.966    \\\\\n\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 97351740.228  \\\\\n\\textbf{Skew:}          &   14.707  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n\\textbf{Kurtosis:}      &  485.278  & \\textbf{  Cond. No.          } &       240.    \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now place the betas we found into the regression and solve for if age was 35 and education level was 11\n",
        "-7.7+ 0.11*35 + 2.65*11\n",
        "\n",
        "#expected hourly wage with those inputs based on this model is $25.299"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbfXqF03tGsm",
        "outputId": "429ba9e6-e33d-447b-e68d-575dbac8ab20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25.299999999999997"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now lets increase the compexcity of the equation\n",
        "#data.columns #look at what other varibales are avaialble to us\n",
        "\n",
        "#want to include age and age^2 (because we know that age is generally non-linear as it rises and then dips as you get older)\n",
        "model = smf.ols(\"hrwage ~ age + I(age**2) + educ\", data=data) #I(age**2)  transforms age into a square\n",
        "modelFit = model.fit()\n",
        "\n",
        "modelFit.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "jKkYKtLrtuRe",
        "outputId": "491a1210-bcd1-4827-ead2-a3b7231ca770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                 hrwage   R-squared:                       0.087\n",
              "Model:                            OLS   Adj. R-squared:                  0.086\n",
              "Method:                 Least Squares   F-statistic:                     316.3\n",
              "Date:                Wed, 04 Dec 2024   Prob (F-statistic):          3.15e-196\n",
              "Time:                        22:10:24   Log-Likelihood:                -43974.\n",
              "No. Observations:               10008   AIC:                         8.796e+04\n",
              "Df Residuals:                   10004   BIC:                         8.799e+04\n",
              "Df Model:                           3                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "===============================================================================\n",
              "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
              "-------------------------------------------------------------------------------\n",
              "Intercept     -23.1554      1.683    -13.756      0.000     -26.455     -19.856\n",
              "age             0.9383      0.074     12.652      0.000       0.793       1.084\n",
              "I(age ** 2)    -0.0090      0.001    -11.250      0.000      -0.011      -0.007\n",
              "educ            2.4954      0.097     25.612      0.000       2.304       2.686\n",
              "==============================================================================\n",
              "Omnibus:                    19537.539   Durbin-Watson:                   1.967\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        104604362.363\n",
              "Skew:                          15.100   Prob(JB):                         0.00\n",
              "Kurtosis:                     502.938   Cond. No.                     2.30e+04\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "[2] The condition number is large, 2.3e+04. This might indicate that there are\n",
              "strong multicollinearity or other numerical problems.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>         <td>hrwage</td>      <th>  R-squared:         </th> <td>   0.087</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.086</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   316.3</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Wed, 04 Dec 2024</td> <th>  Prob (F-statistic):</th> <td>3.15e-196</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>22:10:24</td>     <th>  Log-Likelihood:    </th> <td> -43974.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td> 10008</td>      <th>  AIC:               </th> <td>8.796e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td> 10004</td>      <th>  BIC:               </th> <td>8.799e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th>   <td>  -23.1554</td> <td>    1.683</td> <td>  -13.756</td> <td> 0.000</td> <td>  -26.455</td> <td>  -19.856</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>age</th>         <td>    0.9383</td> <td>    0.074</td> <td>   12.652</td> <td> 0.000</td> <td>    0.793</td> <td>    1.084</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>I(age ** 2)</th> <td>   -0.0090</td> <td>    0.001</td> <td>  -11.250</td> <td> 0.000</td> <td>   -0.011</td> <td>   -0.007</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>educ</th>        <td>    2.4954</td> <td>    0.097</td> <td>   25.612</td> <td> 0.000</td> <td>    2.304</td> <td>    2.686</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>19537.539</td> <th>  Durbin-Watson:     </th>   <td>   1.967</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>104604362.363</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td>15.100</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td>502.938</td>  <th>  Cond. No.          </th>   <td>2.30e+04</td>   \n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.3e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &      hrwage      & \\textbf{  R-squared:         } &       0.087    \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &       0.086    \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &       316.3    \\\\\n\\textbf{Date:}             & Wed, 04 Dec 2024 & \\textbf{  Prob (F-statistic):} &   3.15e-196    \\\\\n\\textbf{Time:}             &     22:10:24     & \\textbf{  Log-Likelihood:    } &     -43974.    \\\\\n\\textbf{No. Observations:} &       10008      & \\textbf{  AIC:               } &   8.796e+04    \\\\\n\\textbf{Df Residuals:}     &       10004      & \\textbf{  BIC:               } &   8.799e+04    \\\\\n\\textbf{Df Model:}         &           3      & \\textbf{                     } &                \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &                \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                     & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept}   &     -23.1554  &        1.683     &   -13.756  &         0.000        &      -26.455    &      -19.856     \\\\\n\\textbf{age}         &       0.9383  &        0.074     &    12.652  &         0.000        &        0.793    &        1.084     \\\\\n\\textbf{I(age ** 2)} &      -0.0090  &        0.001     &   -11.250  &         0.000        &       -0.011    &       -0.007     \\\\\n\\textbf{educ}        &       2.4954  &        0.097     &    25.612  &         0.000        &        2.304    &        2.686     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 19537.539 & \\textbf{  Durbin-Watson:     } &       1.967    \\\\\n\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 104604362.363  \\\\\n\\textbf{Skew:}          &   15.100  & \\textbf{  Prob(JB):          } &        0.00    \\\\\n\\textbf{Kurtosis:}      &  502.938  & \\textbf{  Cond. No.          } &    2.30e+04    \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n [2] The condition number is large, 2.3e+04. This might indicate that there are \\newline\n strong multicollinearity or other numerical problems."
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now place the betas we found into the regression and solve for if age was 35 and education level was 11\n",
        "-7.7 + 0.9383*35 + -.009*35**2 + 2.65 *11\n",
        "\n",
        "#now expected wage is 43.2655 with more complex model and teacher says this is closer to his actual wage so yay\n",
        "\n",
        "#increase the information in our regression gives us a better prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhPievMTuX3-",
        "outputId": "95c345d2-7805-4a30-bbce-0b22ee10eb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43.2655"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#can also do log\n",
        "model = smf.ols(\"np.log(I(hrwage+1)) ~ age + I(age**2) + educ\", data=data) #I(age**2)  transforms age into a square, np.log is showing the model if we took log of hourlywage\n",
        "modelFit = model.fit()\n",
        "\n",
        "modelFit.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "_Xs9WEcAvJCm",
        "outputId": "3d002bf9-bb49-4dba-f6ab-75e64821fb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                              OLS Regression Results                             \n",
              "=================================================================================\n",
              "Dep. Variable:     np.log(I(hrwage + 1))   R-squared:                       0.109\n",
              "Model:                               OLS   Adj. R-squared:                  0.109\n",
              "Method:                    Least Squares   F-statistic:                     409.7\n",
              "Date:                   Wed, 04 Dec 2024   Prob (F-statistic):          4.95e-251\n",
              "Time:                           22:15:14   Log-Likelihood:                -13393.\n",
              "No. Observations:                  10008   AIC:                         2.679e+04\n",
              "Df Residuals:                      10004   BIC:                         2.682e+04\n",
              "Df Model:                              3                                         \n",
              "Covariance Type:               nonrobust                                         \n",
              "===============================================================================\n",
              "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
              "-------------------------------------------------------------------------------\n",
              "Intercept       0.2335      0.079      2.946      0.003       0.078       0.389\n",
              "age             0.0689      0.003     19.718      0.000       0.062       0.076\n",
              "I(age ** 2)    -0.0007   3.78e-05    -19.704      0.000      -0.001      -0.001\n",
              "educ            0.1188      0.005     25.882      0.000       0.110       0.128\n",
              "==============================================================================\n",
              "Omnibus:                     2271.958   Durbin-Watson:                   1.950\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5075.249\n",
              "Skew:                          -1.297   Prob(JB):                         0.00\n",
              "Kurtosis:                       5.333   Cond. No.                     2.30e+04\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "[2] The condition number is large, 2.3e+04. This might indicate that there are\n",
              "strong multicollinearity or other numerical problems.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>    <td>np.log(I(hrwage + 1))</td> <th>  R-squared:         </th> <td>   0.109</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                     <td>OLS</td>          <th>  Adj. R-squared:    </th> <td>   0.109</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>               <td>Least Squares</td>     <th>  F-statistic:       </th> <td>   409.7</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>               <td>Wed, 04 Dec 2024</td>    <th>  Prob (F-statistic):</th> <td>4.95e-251</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                   <td>22:15:14</td>        <th>  Log-Likelihood:    </th> <td> -13393.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>        <td> 10008</td>         <th>  AIC:               </th> <td>2.679e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>            <td> 10004</td>         <th>  BIC:               </th> <td>2.682e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>                <td>     3</td>         <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>        <td>nonrobust</td>       <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th>   <td>    0.2335</td> <td>    0.079</td> <td>    2.946</td> <td> 0.003</td> <td>    0.078</td> <td>    0.389</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>age</th>         <td>    0.0689</td> <td>    0.003</td> <td>   19.718</td> <td> 0.000</td> <td>    0.062</td> <td>    0.076</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>I(age ** 2)</th> <td>   -0.0007</td> <td> 3.78e-05</td> <td>  -19.704</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>educ</th>        <td>    0.1188</td> <td>    0.005</td> <td>   25.882</td> <td> 0.000</td> <td>    0.110</td> <td>    0.128</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>2271.958</td> <th>  Durbin-Watson:     </th> <td>   1.950</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5075.249</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td>-1.297</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td> 5.333</td>  <th>  Cond. No.          </th> <td>2.30e+04</td>\n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.3e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    & np.log(I(hrwage + 1)) & \\textbf{  R-squared:         } &     0.109   \\\\\n\\textbf{Model:}            &          OLS          & \\textbf{  Adj. R-squared:    } &     0.109   \\\\\n\\textbf{Method:}           &     Least Squares     & \\textbf{  F-statistic:       } &     409.7   \\\\\n\\textbf{Date:}             &    Wed, 04 Dec 2024   & \\textbf{  Prob (F-statistic):} & 4.95e-251   \\\\\n\\textbf{Time:}             &        22:15:14       & \\textbf{  Log-Likelihood:    } &   -13393.   \\\\\n\\textbf{No. Observations:} &          10008        & \\textbf{  AIC:               } & 2.679e+04   \\\\\n\\textbf{Df Residuals:}     &          10004        & \\textbf{  BIC:               } & 2.682e+04   \\\\\n\\textbf{Df Model:}         &              3        & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &       nonrobust       & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                     & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept}   &       0.2335  &        0.079     &     2.946  &         0.003        &        0.078    &        0.389     \\\\\n\\textbf{age}         &       0.0689  &        0.003     &    19.718  &         0.000        &        0.062    &        0.076     \\\\\n\\textbf{I(age ** 2)} &      -0.0007  &     3.78e-05     &   -19.704  &         0.000        &       -0.001    &       -0.001     \\\\\n\\textbf{educ}        &       0.1188  &        0.005     &    25.882  &         0.000        &        0.110    &        0.128     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 2271.958 & \\textbf{  Durbin-Watson:     } &    1.950  \\\\\n\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } & 5075.249  \\\\\n\\textbf{Skew:}          &  -1.297  & \\textbf{  Prob(JB):          } &     0.00  \\\\\n\\textbf{Kurtosis:}      &   5.333  & \\textbf{  Cond. No.          } & 2.30e+04  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n [2] The condition number is large, 2.3e+04. This might indicate that there are \\newline\n strong multicollinearity or other numerical problems."
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The C() command indicates that we would like to consider the race variable as a Categorical variable, not a numeric variable.\n",
        "model = smf.ols(\"np.log(I(hrwage+1)) ~ age + I(age**2) + C(educ)\", data=data) #create categorical varibales for every education level, creates nonlinear relationship between education categories\n",
        "modelFit = model.fit()\n",
        "\n",
        "modelFit.summary()\n",
        "\n",
        "#in theoriy since adding the c increase out r-squared a bit this should mean that this mafe rthe model better or sumthin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "APZao4UrvuNW",
        "outputId": "93c97247-24d9-4077-be28-7444fad9dea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                              OLS Regression Results                             \n",
              "=================================================================================\n",
              "Dep. Variable:     np.log(I(hrwage + 1))   R-squared:                       0.118\n",
              "Model:                               OLS   Adj. R-squared:                  0.117\n",
              "Method:                    Least Squares   F-statistic:                     121.7\n",
              "Date:                   Wed, 04 Dec 2024   Prob (F-statistic):          8.85e-263\n",
              "Time:                           22:17:49   Log-Likelihood:                -13344.\n",
              "No. Observations:                  10008   AIC:                         2.671e+04\n",
              "Df Residuals:                       9996   BIC:                         2.680e+04\n",
              "Df Model:                             11                                         \n",
              "Covariance Type:               nonrobust                                         \n",
              "=================================================================================\n",
              "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
              "---------------------------------------------------------------------------------\n",
              "Intercept         0.9770      0.199      4.900      0.000       0.586       1.368\n",
              "C(educ)[T.2]     -0.0713      0.204     -0.350      0.726      -0.470       0.328\n",
              "C(educ)[T.3]      0.0318      0.216      0.147      0.883      -0.391       0.455\n",
              "C(educ)[T.4]     -0.1579      0.211     -0.750      0.453      -0.571       0.255\n",
              "C(educ)[T.5]     -0.1769      0.198     -0.892      0.373      -0.566       0.212\n",
              "C(educ)[T.6]      0.0011      0.184      0.006      0.995      -0.360       0.363\n",
              "C(educ)[T.7]      0.0758      0.185      0.409      0.682      -0.287       0.439\n",
              "C(educ)[T.8]      0.1268      0.186      0.683      0.494      -0.237       0.490\n",
              "C(educ)[T.10]     0.4622      0.185      2.501      0.012       0.100       0.824\n",
              "C(educ)[T.11]     0.7630      0.186      4.099      0.000       0.398       1.128\n",
              "age               0.0678      0.004     19.289      0.000       0.061       0.075\n",
              "I(age ** 2)      -0.0007    3.8e-05    -19.427      0.000      -0.001      -0.001\n",
              "==============================================================================\n",
              "Omnibus:                     2269.664   Durbin-Watson:                   1.952\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5050.066\n",
              "Skew:                          -1.298   Prob(JB):                         0.00\n",
              "Kurtosis:                       5.319   Cond. No.                     1.70e+05\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "[2] The condition number is large, 1.7e+05. This might indicate that there are\n",
              "strong multicollinearity or other numerical problems.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>    <td>np.log(I(hrwage + 1))</td> <th>  R-squared:         </th> <td>   0.118</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                     <td>OLS</td>          <th>  Adj. R-squared:    </th> <td>   0.117</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>               <td>Least Squares</td>     <th>  F-statistic:       </th> <td>   121.7</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>               <td>Wed, 04 Dec 2024</td>    <th>  Prob (F-statistic):</th> <td>8.85e-263</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                   <td>22:17:49</td>        <th>  Log-Likelihood:    </th> <td> -13344.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>        <td> 10008</td>         <th>  AIC:               </th> <td>2.671e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>            <td>  9996</td>         <th>  BIC:               </th> <td>2.680e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>                <td>    11</td>         <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>        <td>nonrobust</td>       <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th>     <td>    0.9770</td> <td>    0.199</td> <td>    4.900</td> <td> 0.000</td> <td>    0.586</td> <td>    1.368</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.2]</th>  <td>   -0.0713</td> <td>    0.204</td> <td>   -0.350</td> <td> 0.726</td> <td>   -0.470</td> <td>    0.328</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.3]</th>  <td>    0.0318</td> <td>    0.216</td> <td>    0.147</td> <td> 0.883</td> <td>   -0.391</td> <td>    0.455</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.4]</th>  <td>   -0.1579</td> <td>    0.211</td> <td>   -0.750</td> <td> 0.453</td> <td>   -0.571</td> <td>    0.255</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.5]</th>  <td>   -0.1769</td> <td>    0.198</td> <td>   -0.892</td> <td> 0.373</td> <td>   -0.566</td> <td>    0.212</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.6]</th>  <td>    0.0011</td> <td>    0.184</td> <td>    0.006</td> <td> 0.995</td> <td>   -0.360</td> <td>    0.363</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.7]</th>  <td>    0.0758</td> <td>    0.185</td> <td>    0.409</td> <td> 0.682</td> <td>   -0.287</td> <td>    0.439</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.8]</th>  <td>    0.1268</td> <td>    0.186</td> <td>    0.683</td> <td> 0.494</td> <td>   -0.237</td> <td>    0.490</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.10]</th> <td>    0.4622</td> <td>    0.185</td> <td>    2.501</td> <td> 0.012</td> <td>    0.100</td> <td>    0.824</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.11]</th> <td>    0.7630</td> <td>    0.186</td> <td>    4.099</td> <td> 0.000</td> <td>    0.398</td> <td>    1.128</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>age</th>           <td>    0.0678</td> <td>    0.004</td> <td>   19.289</td> <td> 0.000</td> <td>    0.061</td> <td>    0.075</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>I(age ** 2)</th>   <td>   -0.0007</td> <td>  3.8e-05</td> <td>  -19.427</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>2269.664</td> <th>  Durbin-Watson:     </th> <td>   1.952</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5050.066</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td>-1.298</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td> 5.319</td>  <th>  Cond. No.          </th> <td>1.70e+05</td>\n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.7e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    & np.log(I(hrwage + 1)) & \\textbf{  R-squared:         } &     0.118   \\\\\n\\textbf{Model:}            &          OLS          & \\textbf{  Adj. R-squared:    } &     0.117   \\\\\n\\textbf{Method:}           &     Least Squares     & \\textbf{  F-statistic:       } &     121.7   \\\\\n\\textbf{Date:}             &    Wed, 04 Dec 2024   & \\textbf{  Prob (F-statistic):} & 8.85e-263   \\\\\n\\textbf{Time:}             &        22:17:49       & \\textbf{  Log-Likelihood:    } &   -13344.   \\\\\n\\textbf{No. Observations:} &          10008        & \\textbf{  AIC:               } & 2.671e+04   \\\\\n\\textbf{Df Residuals:}     &           9996        & \\textbf{  BIC:               } & 2.680e+04   \\\\\n\\textbf{Df Model:}         &             11        & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &       nonrobust       & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept}     &       0.9770  &        0.199     &     4.900  &         0.000        &        0.586    &        1.368     \\\\\n\\textbf{C(educ)[T.2]}  &      -0.0713  &        0.204     &    -0.350  &         0.726        &       -0.470    &        0.328     \\\\\n\\textbf{C(educ)[T.3]}  &       0.0318  &        0.216     &     0.147  &         0.883        &       -0.391    &        0.455     \\\\\n\\textbf{C(educ)[T.4]}  &      -0.1579  &        0.211     &    -0.750  &         0.453        &       -0.571    &        0.255     \\\\\n\\textbf{C(educ)[T.5]}  &      -0.1769  &        0.198     &    -0.892  &         0.373        &       -0.566    &        0.212     \\\\\n\\textbf{C(educ)[T.6]}  &       0.0011  &        0.184     &     0.006  &         0.995        &       -0.360    &        0.363     \\\\\n\\textbf{C(educ)[T.7]}  &       0.0758  &        0.185     &     0.409  &         0.682        &       -0.287    &        0.439     \\\\\n\\textbf{C(educ)[T.8]}  &       0.1268  &        0.186     &     0.683  &         0.494        &       -0.237    &        0.490     \\\\\n\\textbf{C(educ)[T.10]} &       0.4622  &        0.185     &     2.501  &         0.012        &        0.100    &        0.824     \\\\\n\\textbf{C(educ)[T.11]} &       0.7630  &        0.186     &     4.099  &         0.000        &        0.398    &        1.128     \\\\\n\\textbf{age}           &       0.0678  &        0.004     &    19.289  &         0.000        &        0.061    &        0.075     \\\\\n\\textbf{I(age ** 2)}   &      -0.0007  &      3.8e-05     &   -19.427  &         0.000        &       -0.001    &       -0.001     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 2269.664 & \\textbf{  Durbin-Watson:     } &    1.952  \\\\\n\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } & 5050.066  \\\\\n\\textbf{Skew:}          &  -1.298  & \\textbf{  Prob(JB):          } &     0.00  \\\\\n\\textbf{Kurtosis:}      &   5.319  & \\textbf{  Cond. No.          } & 1.70e+05  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n [2] The condition number is large, 1.7e+05. This might indicate that there are \\newline\n strong multicollinearity or other numerical problems."
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#robust standard errors\n",
        "\n",
        "modelFit.get_robustcov_results(cov_type ='HC0').summary()\n",
        "\n",
        "#now standard errors are more robust to protect against heteroskedasticity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "-plcHrqswUUy",
        "outputId": "e39963f8-1171-48f4-b992-c823991a66ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                              OLS Regression Results                             \n",
              "=================================================================================\n",
              "Dep. Variable:     np.log(I(hrwage + 1))   R-squared:                       0.118\n",
              "Model:                               OLS   Adj. R-squared:                  0.117\n",
              "Method:                    Least Squares   F-statistic:                     133.0\n",
              "Date:                   Wed, 04 Dec 2024   Prob (F-statistic):          3.29e-286\n",
              "Time:                           22:20:33   Log-Likelihood:                -13344.\n",
              "No. Observations:                  10008   AIC:                         2.671e+04\n",
              "Df Residuals:                       9996   BIC:                         2.680e+04\n",
              "Df Model:                             11                                         \n",
              "Covariance Type:                     HC0                                         \n",
              "=================================================================================\n",
              "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
              "---------------------------------------------------------------------------------\n",
              "Intercept         0.9770      0.152      6.427      0.000       0.679       1.275\n",
              "C(educ)[T.2]     -0.0713      0.156     -0.457      0.648      -0.377       0.235\n",
              "C(educ)[T.3]      0.0318      0.155      0.205      0.838      -0.273       0.336\n",
              "C(educ)[T.4]     -0.1579      0.164     -0.966      0.334      -0.478       0.163\n",
              "C(educ)[T.5]     -0.1769      0.146     -1.210      0.226      -0.464       0.110\n",
              "C(educ)[T.6]      0.0011      0.134      0.008      0.993      -0.261       0.263\n",
              "C(educ)[T.7]      0.0758      0.135      0.563      0.573      -0.188       0.340\n",
              "C(educ)[T.8]      0.1268      0.135      0.936      0.349      -0.139       0.392\n",
              "C(educ)[T.10]     0.4622      0.134      3.450      0.001       0.200       0.725\n",
              "C(educ)[T.11]     0.7630      0.136      5.622      0.000       0.497       1.029\n",
              "age               0.0678      0.004     18.602      0.000       0.061       0.075\n",
              "I(age ** 2)      -0.0007   4.19e-05    -17.607      0.000      -0.001      -0.001\n",
              "==============================================================================\n",
              "Omnibus:                     2269.664   Durbin-Watson:                   1.952\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5050.066\n",
              "Skew:                          -1.298   Prob(JB):                         0.00\n",
              "Kurtosis:                       5.319   Cond. No.                     1.70e+05\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors are heteroscedasticity robust (HC0)\n",
              "[2] The condition number is large, 1.7e+05. This might indicate that there are\n",
              "strong multicollinearity or other numerical problems.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>    <td>np.log(I(hrwage + 1))</td> <th>  R-squared:         </th> <td>   0.118</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                     <td>OLS</td>          <th>  Adj. R-squared:    </th> <td>   0.117</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>               <td>Least Squares</td>     <th>  F-statistic:       </th> <td>   133.0</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>               <td>Wed, 04 Dec 2024</td>    <th>  Prob (F-statistic):</th> <td>3.29e-286</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                   <td>22:20:33</td>        <th>  Log-Likelihood:    </th> <td> -13344.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>        <td> 10008</td>         <th>  AIC:               </th> <td>2.671e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>            <td>  9996</td>         <th>  BIC:               </th> <td>2.680e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>                <td>    11</td>         <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>           <td>HC0</td>          <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th>     <td>    0.9770</td> <td>    0.152</td> <td>    6.427</td> <td> 0.000</td> <td>    0.679</td> <td>    1.275</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.2]</th>  <td>   -0.0713</td> <td>    0.156</td> <td>   -0.457</td> <td> 0.648</td> <td>   -0.377</td> <td>    0.235</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.3]</th>  <td>    0.0318</td> <td>    0.155</td> <td>    0.205</td> <td> 0.838</td> <td>   -0.273</td> <td>    0.336</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.4]</th>  <td>   -0.1579</td> <td>    0.164</td> <td>   -0.966</td> <td> 0.334</td> <td>   -0.478</td> <td>    0.163</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.5]</th>  <td>   -0.1769</td> <td>    0.146</td> <td>   -1.210</td> <td> 0.226</td> <td>   -0.464</td> <td>    0.110</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.6]</th>  <td>    0.0011</td> <td>    0.134</td> <td>    0.008</td> <td> 0.993</td> <td>   -0.261</td> <td>    0.263</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.7]</th>  <td>    0.0758</td> <td>    0.135</td> <td>    0.563</td> <td> 0.573</td> <td>   -0.188</td> <td>    0.340</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.8]</th>  <td>    0.1268</td> <td>    0.135</td> <td>    0.936</td> <td> 0.349</td> <td>   -0.139</td> <td>    0.392</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.10]</th> <td>    0.4622</td> <td>    0.134</td> <td>    3.450</td> <td> 0.001</td> <td>    0.200</td> <td>    0.725</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.11]</th> <td>    0.7630</td> <td>    0.136</td> <td>    5.622</td> <td> 0.000</td> <td>    0.497</td> <td>    1.029</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>age</th>           <td>    0.0678</td> <td>    0.004</td> <td>   18.602</td> <td> 0.000</td> <td>    0.061</td> <td>    0.075</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>I(age ** 2)</th>   <td>   -0.0007</td> <td> 4.19e-05</td> <td>  -17.607</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>2269.664</td> <th>  Durbin-Watson:     </th> <td>   1.952</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5050.066</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td>-1.298</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td> 5.319</td>  <th>  Cond. No.          </th> <td>1.70e+05</td>\n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC0)<br/>[2] The condition number is large, 1.7e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    & np.log(I(hrwage + 1)) & \\textbf{  R-squared:         } &     0.118   \\\\\n\\textbf{Model:}            &          OLS          & \\textbf{  Adj. R-squared:    } &     0.117   \\\\\n\\textbf{Method:}           &     Least Squares     & \\textbf{  F-statistic:       } &     133.0   \\\\\n\\textbf{Date:}             &    Wed, 04 Dec 2024   & \\textbf{  Prob (F-statistic):} & 3.29e-286   \\\\\n\\textbf{Time:}             &        22:20:33       & \\textbf{  Log-Likelihood:    } &   -13344.   \\\\\n\\textbf{No. Observations:} &          10008        & \\textbf{  AIC:               } & 2.671e+04   \\\\\n\\textbf{Df Residuals:}     &           9996        & \\textbf{  BIC:               } & 2.680e+04   \\\\\n\\textbf{Df Model:}         &             11        & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &          HC0          & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept}     &       0.9770  &        0.152     &     6.427  &         0.000        &        0.679    &        1.275     \\\\\n\\textbf{C(educ)[T.2]}  &      -0.0713  &        0.156     &    -0.457  &         0.648        &       -0.377    &        0.235     \\\\\n\\textbf{C(educ)[T.3]}  &       0.0318  &        0.155     &     0.205  &         0.838        &       -0.273    &        0.336     \\\\\n\\textbf{C(educ)[T.4]}  &      -0.1579  &        0.164     &    -0.966  &         0.334        &       -0.478    &        0.163     \\\\\n\\textbf{C(educ)[T.5]}  &      -0.1769  &        0.146     &    -1.210  &         0.226        &       -0.464    &        0.110     \\\\\n\\textbf{C(educ)[T.6]}  &       0.0011  &        0.134     &     0.008  &         0.993        &       -0.261    &        0.263     \\\\\n\\textbf{C(educ)[T.7]}  &       0.0758  &        0.135     &     0.563  &         0.573        &       -0.188    &        0.340     \\\\\n\\textbf{C(educ)[T.8]}  &       0.1268  &        0.135     &     0.936  &         0.349        &       -0.139    &        0.392     \\\\\n\\textbf{C(educ)[T.10]} &       0.4622  &        0.134     &     3.450  &         0.001        &        0.200    &        0.725     \\\\\n\\textbf{C(educ)[T.11]} &       0.7630  &        0.136     &     5.622  &         0.000        &        0.497    &        1.029     \\\\\n\\textbf{age}           &       0.0678  &        0.004     &    18.602  &         0.000        &        0.061    &        0.075     \\\\\n\\textbf{I(age ** 2)}   &      -0.0007  &     4.19e-05     &   -17.607  &         0.000        &       -0.001    &       -0.001     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 2269.664 & \\textbf{  Durbin-Watson:     } &    1.952  \\\\\n\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } & 5050.066  \\\\\n\\textbf{Skew:}          &  -1.298  & \\textbf{  Prob(JB):          } &     0.00  \\\\\n\\textbf{Kurtosis:}      &   5.319  & \\textbf{  Cond. No.          } & 1.70e+05  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors are heteroscedasticity robust (HC0) \\newline\n [2] The condition number is large, 1.7e+05. This might indicate that there are \\newline\n strong multicollinearity or other numerical problems."
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OTHER STATSMODEL MODELING OPTIONS"
      ],
      "metadata": {
        "id": "FXOvJF-RxIO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Modeling descrete outcomes\n",
        "\n",
        "#Logit and Probit\n",
        "\n",
        "#lets try a logistic regression instead of ols this time\n",
        "\n",
        "#try to predict marrage based on number of children under 5\n",
        "model = smf.logit(\"married ~ nchlt5\", data=data)\n",
        "modelFit = model.fit()\n",
        "\n",
        "modelFit.summary()\n",
        "\n",
        "#tells that # of children has a strong positive relationship with whether someone is married (so having children under 5 is a good indicator for predicting is someone is married)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "kZTN2BdFxMeq",
        "outputId": "1eaf1dd7-1722-437b-8dc6-121b5a63f54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.634598\n",
            "         Iterations 6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                           Logit Regression Results                           \n",
              "==============================================================================\n",
              "Dep. Variable:                married   No. Observations:                13712\n",
              "Model:                          Logit   Df Residuals:                    13710\n",
              "Method:                           MLE   Df Model:                            1\n",
              "Date:                Wed, 04 Dec 2024   Pseudo R-squ.:                 0.02816\n",
              "Time:                        22:26:00   Log-Likelihood:                -8701.6\n",
              "converged:                       True   LL-Null:                       -8953.7\n",
              "Covariance Type:            nonrobust   LLR p-value:                1.147e-111\n",
              "==============================================================================\n",
              "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept      0.4602      0.018     24.906      0.000       0.424       0.496\n",
              "nchlt5         1.2554      0.072     17.552      0.000       1.115       1.396\n",
              "==============================================================================\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Logit Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>        <td>married</td>     <th>  No. Observations:  </th>   <td> 13712</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td> 13710</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Wed, 04 Dec 2024</td> <th>  Pseudo R-squ.:     </th>   <td>0.02816</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>22:26:00</td>     <th>  Log-Likelihood:    </th>  <td> -8701.6</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -8953.7</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.147e-111</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>    0.4602</td> <td>    0.018</td> <td>   24.906</td> <td> 0.000</td> <td>    0.424</td> <td>    0.496</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>nchlt5</th>    <td>    1.2554</td> <td>    0.072</td> <td>   17.552</td> <td> 0.000</td> <td>    1.115</td> <td>    1.396</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}   &     married      & \\textbf{  No. Observations:  } &    13712    \\\\\n\\textbf{Model:}           &      Logit       & \\textbf{  Df Residuals:      } &    13710    \\\\\n\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        1    \\\\\n\\textbf{Date:}            & Wed, 04 Dec 2024 & \\textbf{  Pseudo R-squ.:     } &  0.02816    \\\\\n\\textbf{Time:}            &     22:26:00     & \\textbf{  Log-Likelihood:    } &   -8701.6   \\\\\n\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -8953.7   \\\\\n\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } & 1.147e-111  \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                   & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept} &       0.4602  &        0.018     &    24.906  &         0.000        &        0.424    &        0.496     \\\\\n\\textbf{nchlt5}    &       1.2554  &        0.072     &    17.552  &         0.000        &        1.115    &        1.396     \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{Logit Regression Results}\n\\end{center}"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add to see how different educational level (made categorical) are associated with marital status\n",
        "model = smf.logit(\"married ~ nchlt5 + C(educ)\", data=data)\n",
        "modelFit = model.fit()\n",
        "\n",
        "modelFit.summary()\n",
        "#we can see that everything after T.8 (which is highschool graduuate) has positive coefficient\n",
        "#this would mean that being a highschool grad or more steadily increases the likelyhood of somebody being married"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "fYUktY16yYU9",
        "outputId": "f4b90748-4566-4569-a0bc-800f564838d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.624782\n",
            "         Iterations 6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                           Logit Regression Results                           \n",
              "==============================================================================\n",
              "Dep. Variable:                married   No. Observations:                13712\n",
              "Model:                          Logit   Df Residuals:                    13701\n",
              "Method:                           MLE   Df Model:                           10\n",
              "Date:                Wed, 04 Dec 2024   Pseudo R-squ.:                 0.04319\n",
              "Time:                        22:28:16   Log-Likelihood:                -8567.0\n",
              "converged:                       True   LL-Null:                       -8953.7\n",
              "Covariance Type:            nonrobust   LLR p-value:                1.073e-159\n",
              "=================================================================================\n",
              "                    coef    std err          z      P>|z|      [0.025      0.975]\n",
              "---------------------------------------------------------------------------------\n",
              "Intercept         0.4283      0.330      1.298      0.194      -0.219       1.075\n",
              "C(educ)[T.2]     -0.1499      0.355     -0.422      0.673      -0.846       0.547\n",
              "C(educ)[T.3]      0.0487      0.386      0.126      0.899      -0.707       0.804\n",
              "C(educ)[T.4]     -0.3370      0.366     -0.921      0.357      -1.054       0.380\n",
              "C(educ)[T.5]     -0.9590      0.353     -2.716      0.007      -1.651      -0.267\n",
              "C(educ)[T.6]     -0.0678      0.331     -0.205      0.838      -0.717       0.581\n",
              "C(educ)[T.7]     -0.1913      0.333     -0.574      0.566      -0.844       0.461\n",
              "C(educ)[T.8]      0.2012      0.335      0.601      0.548      -0.455       0.858\n",
              "C(educ)[T.10]     0.2421      0.333      0.727      0.467      -0.410       0.895\n",
              "C(educ)[T.11]     0.8236      0.338      2.434      0.015       0.160       1.487\n",
              "nchlt5            1.1981      0.072     16.689      0.000       1.057       1.339\n",
              "=================================================================================\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Logit Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>        <td>married</td>     <th>  No. Observations:  </th>   <td> 13712</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td> 13701</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    10</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Wed, 04 Dec 2024</td> <th>  Pseudo R-squ.:     </th>   <td>0.04319</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>22:28:16</td>     <th>  Log-Likelihood:    </th>  <td> -8567.0</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -8953.7</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.073e-159</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "        <td></td>           <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th>     <td>    0.4283</td> <td>    0.330</td> <td>    1.298</td> <td> 0.194</td> <td>   -0.219</td> <td>    1.075</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.2]</th>  <td>   -0.1499</td> <td>    0.355</td> <td>   -0.422</td> <td> 0.673</td> <td>   -0.846</td> <td>    0.547</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.3]</th>  <td>    0.0487</td> <td>    0.386</td> <td>    0.126</td> <td> 0.899</td> <td>   -0.707</td> <td>    0.804</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.4]</th>  <td>   -0.3370</td> <td>    0.366</td> <td>   -0.921</td> <td> 0.357</td> <td>   -1.054</td> <td>    0.380</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.5]</th>  <td>   -0.9590</td> <td>    0.353</td> <td>   -2.716</td> <td> 0.007</td> <td>   -1.651</td> <td>   -0.267</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.6]</th>  <td>   -0.0678</td> <td>    0.331</td> <td>   -0.205</td> <td> 0.838</td> <td>   -0.717</td> <td>    0.581</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.7]</th>  <td>   -0.1913</td> <td>    0.333</td> <td>   -0.574</td> <td> 0.566</td> <td>   -0.844</td> <td>    0.461</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.8]</th>  <td>    0.2012</td> <td>    0.335</td> <td>    0.601</td> <td> 0.548</td> <td>   -0.455</td> <td>    0.858</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.10]</th> <td>    0.2421</td> <td>    0.333</td> <td>    0.727</td> <td> 0.467</td> <td>   -0.410</td> <td>    0.895</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>C(educ)[T.11]</th> <td>    0.8236</td> <td>    0.338</td> <td>    2.434</td> <td> 0.015</td> <td>    0.160</td> <td>    1.487</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>nchlt5</th>        <td>    1.1981</td> <td>    0.072</td> <td>   16.689</td> <td> 0.000</td> <td>    1.057</td> <td>    1.339</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}   &     married      & \\textbf{  No. Observations:  } &    13712    \\\\\n\\textbf{Model:}           &      Logit       & \\textbf{  Df Residuals:      } &    13701    \\\\\n\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &       10    \\\\\n\\textbf{Date:}            & Wed, 04 Dec 2024 & \\textbf{  Pseudo R-squ.:     } &  0.04319    \\\\\n\\textbf{Time:}            &     22:28:16     & \\textbf{  Log-Likelihood:    } &   -8567.0   \\\\\n\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -8953.7   \\\\\n\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } & 1.073e-159  \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                       & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept}     &       0.4283  &        0.330     &     1.298  &         0.194        &       -0.219    &        1.075     \\\\\n\\textbf{C(educ)[T.2]}  &      -0.1499  &        0.355     &    -0.422  &         0.673        &       -0.846    &        0.547     \\\\\n\\textbf{C(educ)[T.3]}  &       0.0487  &        0.386     &     0.126  &         0.899        &       -0.707    &        0.804     \\\\\n\\textbf{C(educ)[T.4]}  &      -0.3370  &        0.366     &    -0.921  &         0.357        &       -1.054    &        0.380     \\\\\n\\textbf{C(educ)[T.5]}  &      -0.9590  &        0.353     &    -2.716  &         0.007        &       -1.651    &       -0.267     \\\\\n\\textbf{C(educ)[T.6]}  &      -0.0678  &        0.331     &    -0.205  &         0.838        &       -0.717    &        0.581     \\\\\n\\textbf{C(educ)[T.7]}  &      -0.1913  &        0.333     &    -0.574  &         0.566        &       -0.844    &        0.461     \\\\\n\\textbf{C(educ)[T.8]}  &       0.2012  &        0.335     &     0.601  &         0.548        &       -0.455    &        0.858     \\\\\n\\textbf{C(educ)[T.10]} &       0.2421  &        0.333     &     0.727  &         0.467        &       -0.410    &        0.895     \\\\\n\\textbf{C(educ)[T.11]} &       0.8236  &        0.338     &     2.434  &         0.015        &        0.160    &        1.487     \\\\\n\\textbf{nchlt5}        &       1.1981  &        0.072     &    16.689  &         0.000        &        1.057    &        1.339     \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{Logit Regression Results}\n\\end{center}"
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can also use count models (we dont just have to use binary models)\n",
        "\n",
        "#now trying to see how well hourly wage can predict the number of children under 5\n",
        "model = smf.poisson(\"nchlt5~ hrwage\", data=data[data['nchlt5']>0]) #the last part is making so we are only looking at the data where number of children under 5 is greaeter than\n",
        "#zero idk why but for some reason poisson cannot take this variable unless it is non-zero\n",
        "\n",
        "modelFit = model.fit()\n",
        "\n",
        "modelFit.summary()\n",
        "\n",
        "#hourlywafge increase have a small positive effect on the number of children (.0005), so its has a little affect but not statistically significant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "CvmsdgmVzv4T",
        "outputId": "10989c8f-288f-4a86-d2f6-da6b962bb1a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 1.198935\n",
            "         Iterations 4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                          Poisson Regression Results                          \n",
              "==============================================================================\n",
              "Dep. Variable:                 nchlt5   No. Observations:                 1259\n",
              "Model:                        Poisson   Df Residuals:                     1257\n",
              "Method:                           MLE   Df Model:                            1\n",
              "Date:                Wed, 04 Dec 2024   Pseudo R-squ.:               3.527e-05\n",
              "Time:                        22:39:33   Log-Likelihood:                -1509.5\n",
              "converged:                       True   LL-Null:                       -1509.5\n",
              "Covariance Type:            nonrobust   LLR p-value:                    0.7442\n",
              "==============================================================================\n",
              "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept      0.2807      0.037      7.638      0.000       0.209       0.353\n",
              "hrwage         0.0005      0.001      0.329      0.742      -0.002       0.003\n",
              "==============================================================================\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Poisson Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>        <td>nchlt5</td>      <th>  No. Observations:  </th>  <td>  1259</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                <td>Poisson</td>     <th>  Df Residuals:      </th>  <td>  1257</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Wed, 04 Dec 2024</td> <th>  Pseudo R-squ.:     </th> <td>3.527e-05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>22:39:33</td>     <th>  Log-Likelihood:    </th> <td> -1509.5</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -1509.5</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.7442</td>  \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>    0.2807</td> <td>    0.037</td> <td>    7.638</td> <td> 0.000</td> <td>    0.209</td> <td>    0.353</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>hrwage</th>    <td>    0.0005</td> <td>    0.001</td> <td>    0.329</td> <td> 0.742</td> <td>   -0.002</td> <td>    0.003</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}   &      nchlt5      & \\textbf{  No. Observations:  } &     1259    \\\\\n\\textbf{Model:}           &     Poisson      & \\textbf{  Df Residuals:      } &     1257    \\\\\n\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        1    \\\\\n\\textbf{Date:}            & Wed, 04 Dec 2024 & \\textbf{  Pseudo R-squ.:     } & 3.527e-05   \\\\\n\\textbf{Time:}            &     22:39:33     & \\textbf{  Log-Likelihood:    } &   -1509.5   \\\\\n\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -1509.5   \\\\\n\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } &   0.7442    \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                   & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept} &       0.2807  &        0.037     &     7.638  &         0.000        &        0.209    &        0.353     \\\\\n\\textbf{hrwage}    &       0.0005  &        0.001     &     0.329  &         0.742        &       -0.002    &        0.003     \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{Poisson Regression Results}\n\\end{center}"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "USING THE PATSY LIBRARY: Using regression equations"
      ],
      "metadata": {
        "id": "2tA17yWp1-nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets us the use for stats models and machine learning for building out our matrixes\n",
        "\n",
        "#lets us create a pipeilne where we can put future data in identical structure (basically it will adjust itself around any changes in new)\n",
        "\n",
        "\n",
        "\n",
        "import statsmodels.formula.api as smf\n",
        "import patsy as pt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")\n",
        "\n",
        "# To create y AND x matrices\n",
        "y, x = pt.dmatrices(\"hhincome ~ married + nchlt5 + educ\", data = data)\n",
        "\n"
      ],
      "metadata": {
        "id": "aisJIL_X2B2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEx27j9l6zyQ",
        "outputId": "500c85d2-d8fd-41d7-8fd5-6162b694e460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DesignMatrix with shape (13712, 1)\n",
              "  hhincome\n",
              "     44000\n",
              "     44000\n",
              "     15000\n",
              "     72000\n",
              "     72000\n",
              "     73560\n",
              "     73560\n",
              "    119500\n",
              "    119500\n",
              "    146200\n",
              "    146200\n",
              "     60000\n",
              "    159000\n",
              "    159000\n",
              "     16800\n",
              "     71000\n",
              "     71000\n",
              "     57400\n",
              "     57400\n",
              "    119700\n",
              "    119700\n",
              "     80200\n",
              "     80200\n",
              "     24600\n",
              "     52450\n",
              "     52450\n",
              "     52450\n",
              "     66000\n",
              "     66000\n",
              "     49900\n",
              "  [13682 rows omitted]\n",
              "  Terms:\n",
              "    'hhincome' (column 0)\n",
              "  (to view full data, use np.asarray(this_obj))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x #same number of rows as y but 4 columsn even though we only passed through 3 variables. This because the first column is the intercept"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDmK6Yyv7IoR",
        "outputId": "0efc8b24-216c-45bc-e2c3-1a5d2a98a1c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DesignMatrix with shape (13712, 4)\n",
              "  Intercept  married  nchlt5  educ\n",
              "          1        1       0     7\n",
              "          1        1       0     6\n",
              "          1        0       0     4\n",
              "          1        1       0     6\n",
              "          1        1       0     8\n",
              "          1        1       0    11\n",
              "          1        1       0    10\n",
              "          1        1       0     7\n",
              "          1        1       0     7\n",
              "          1        1       0     7\n",
              "          1        1       0     6\n",
              "          1        0       0     7\n",
              "          1        1       0     8\n",
              "          1        1       0    11\n",
              "          1        0       0     6\n",
              "          1        1       0    10\n",
              "          1        1       0    11\n",
              "          1        1       0     6\n",
              "          1        1       0     6\n",
              "          1        1       0     7\n",
              "          1        1       0    10\n",
              "          1        1       0     6\n",
              "          1        1       0     6\n",
              "          1        0       0     4\n",
              "          1        1       1     6\n",
              "          1        1       1     6\n",
              "          1        0       0     7\n",
              "          1        1       1     8\n",
              "          1        1       1     8\n",
              "          1        0       0     6\n",
              "  [13682 rows omitted]\n",
              "  Terms:\n",
              "    'Intercept' (column 0)\n",
              "    'married' (column 1)\n",
              "    'nchlt5' (column 2)\n",
              "    'educ' (column 3)\n",
              "  (to view full data, use np.asarray(this_obj))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To create ONLY an x matrix\n",
        "#x = pt.dmatrix(\"~ year + educ + married + age\",\n",
        "\t\t#data = data)"
      ],
      "metadata": {
        "id": "Dbk8-2rz6yui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can now use these to d matrix algebra\n",
        "\n",
        "x.T @ x #x transposed at x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0fxdUQH9Vyr",
        "outputId": "2de51705-f271-44c4-a987-3a3e0b2c0602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 13712.,   8786.,   1916., 101108.],\n",
              "       [  8786.,   8786.,   1727.,  66477.],\n",
              "       [  1916.,   1727.,   3046.,  15496.],\n",
              "       [101108.,  66477.,  15496., 804014.]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.inv(x.T @ x) @ x.T @ y #this will give us back the beta coefs in our ols model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVsJCTWv9hSz",
        "outputId": "036e8266-f9fe-4e2d-b272-efb1a718835c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-6294.29708105],\n",
              "       [27697.520517  ],\n",
              "       [-7336.06481466],\n",
              "       [ 8925.63788719]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "model = sm.OLS(y,x)\n",
        "modelFit = model.fit()\n",
        "\n",
        "modelFit.summary() #now we can run the regression using the y and x and now we can use them in any other model (not just OLS) thanks to patsy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "4khvf0n294-I",
        "outputId": "742ace21-e6ce-4a35-e920-c2654678b30c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:               hhincome   R-squared:                       0.135\n",
              "Model:                            OLS   Adj. R-squared:                  0.135\n",
              "Method:                 Least Squares   F-statistic:                     711.3\n",
              "Date:                Wed, 04 Dec 2024   Prob (F-statistic):               0.00\n",
              "Time:                        23:18:40   Log-Likelihood:            -1.7032e+05\n",
              "No. Observations:               13712   AIC:                         3.406e+05\n",
              "Df Residuals:                   13708   BIC:                         3.407e+05\n",
              "Df Model:                           3                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept  -6294.2971   1958.258     -3.214      0.001   -1.01e+04   -2455.843\n",
              "married      2.77e+04   1090.050     25.409      0.000    2.56e+04    2.98e+04\n",
              "nchlt5     -7336.0648   1159.508     -6.327      0.000   -9608.859   -5063.271\n",
              "educ        8925.6379    251.076     35.549      0.000    8433.494    9417.782\n",
              "==============================================================================\n",
              "Omnibus:                    10373.944   Durbin-Watson:                   1.058\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           340766.392\n",
              "Skew:                           3.325   Prob(JB):                         0.00\n",
              "Kurtosis:                      26.500   Cond. No.                         30.2\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>        <td>hhincome</td>     <th>  R-squared:         </th>  <td>   0.135</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.135</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   711.3</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Wed, 04 Dec 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>23:18:40</td>     <th>  Log-Likelihood:    </th> <td>-1.7032e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td> 13712</td>      <th>  AIC:               </th>  <td>3.406e+05</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td> 13708</td>      <th>  BIC:               </th>  <td>3.407e+05</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>-6294.2971</td> <td> 1958.258</td> <td>   -3.214</td> <td> 0.001</td> <td>-1.01e+04</td> <td>-2455.843</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>married</th>   <td>  2.77e+04</td> <td> 1090.050</td> <td>   25.409</td> <td> 0.000</td> <td> 2.56e+04</td> <td> 2.98e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>nchlt5</th>    <td>-7336.0648</td> <td> 1159.508</td> <td>   -6.327</td> <td> 0.000</td> <td>-9608.859</td> <td>-5063.271</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>educ</th>      <td> 8925.6379</td> <td>  251.076</td> <td>   35.549</td> <td> 0.000</td> <td> 8433.494</td> <td> 9417.782</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>10373.944</td> <th>  Durbin-Watson:     </th>  <td>   1.058</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>340766.392</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td> 3.325</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td>26.500</td>   <th>  Cond. No.          </th>  <td>    30.2</td> \n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &     hhincome     & \\textbf{  R-squared:         } &      0.135   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.135   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      711.3   \\\\\n\\textbf{Date:}             & Wed, 04 Dec 2024 & \\textbf{  Prob (F-statistic):} &      0.00    \\\\\n\\textbf{Time:}             &     23:18:40     & \\textbf{  Log-Likelihood:    } & -1.7032e+05  \\\\\n\\textbf{No. Observations:} &       13712      & \\textbf{  AIC:               } &  3.406e+05   \\\\\n\\textbf{Df Residuals:}     &       13708      & \\textbf{  BIC:               } &  3.407e+05   \\\\\n\\textbf{Df Model:}         &           3      & \\textbf{                     } &              \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &              \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept} &   -6294.2971  &     1958.258     &    -3.214  &         0.001        &    -1.01e+04    &    -2455.843     \\\\\n\\textbf{married}   &     2.77e+04  &     1090.050     &    25.409  &         0.000        &     2.56e+04    &     2.98e+04     \\\\\n\\textbf{nchlt5}    &   -7336.0648  &     1159.508     &    -6.327  &         0.000        &    -9608.859    &    -5063.271     \\\\\n\\textbf{educ}      &    8925.6379  &      251.076     &    35.549  &         0.000        &     8433.494    &     9417.782     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 10373.944 & \\textbf{  Durbin-Watson:     } &     1.058   \\\\\n\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 340766.392  \\\\\n\\textbf{Skew:}          &    3.325  & \\textbf{  Prob(JB):          } &      0.00   \\\\\n\\textbf{Kurtosis:}      &   26.500  & \\textbf{  Cond. No.          } &      30.2   \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOW we are going to pass new data and build the same design matrix SUPER IMPORTANT"
      ],
      "metadata": {
        "id": "GF77zKRe-QCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#made 2 new sample datas from our original data\n",
        "data1 = data.sample(1000)\n",
        "data2 = data.sample(1000)\n",
        "\n",
        "#now we'll maek regression run on the first datat set\n",
        "y, x = pt.dmatrices(\"hhincome ~ married + nchlt5 + educ\", data = data1)\n",
        "\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "model = sm.OLS(y,x).fit()\n",
        "#modelFit = model.fit()\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "RZMNaYg7-W3D",
        "outputId": "335f5bdc-354c-48ca-e6af-da1a50765b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:               hhincome   R-squared:                       0.111\n",
              "Model:                            OLS   Adj. R-squared:                  0.108\n",
              "Method:                 Least Squares   F-statistic:                     41.45\n",
              "Date:                Wed, 04 Dec 2024   Prob (F-statistic):           3.03e-25\n",
              "Time:                        23:43:30   Log-Likelihood:                -12393.\n",
              "No. Observations:                1000   AIC:                         2.479e+04\n",
              "Df Residuals:                     996   BIC:                         2.481e+04\n",
              "Df Model:                           3                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept   1991.4203   7049.300      0.282      0.778   -1.18e+04    1.58e+04\n",
              "married     2.275e+04   3971.744      5.729      0.000     1.5e+04    3.05e+04\n",
              "nchlt5     -6229.1414   4232.843     -1.472      0.141   -1.45e+04    2077.172\n",
              "educ        8069.8972    898.051      8.986      0.000    6307.607    9832.187\n",
              "==============================================================================\n",
              "Omnibus:                      613.565   Durbin-Watson:                   1.911\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6986.455\n",
              "Skew:                           2.656   Prob(JB):                         0.00\n",
              "Kurtosis:                      14.809   Cond. No.                         30.1\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>        <td>hhincome</td>     <th>  R-squared:         </th> <td>   0.111</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.108</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   41.45</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Wed, 04 Dec 2024</td> <th>  Prob (F-statistic):</th> <td>3.03e-25</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>23:43:30</td>     <th>  Log-Likelihood:    </th> <td> -12393.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>2.479e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>   996</td>      <th>  BIC:               </th> <td>2.481e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td> 1991.4203</td> <td> 7049.300</td> <td>    0.282</td> <td> 0.778</td> <td>-1.18e+04</td> <td> 1.58e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>married</th>   <td> 2.275e+04</td> <td> 3971.744</td> <td>    5.729</td> <td> 0.000</td> <td>  1.5e+04</td> <td> 3.05e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>nchlt5</th>    <td>-6229.1414</td> <td> 4232.843</td> <td>   -1.472</td> <td> 0.141</td> <td>-1.45e+04</td> <td> 2077.172</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>educ</th>      <td> 8069.8972</td> <td>  898.051</td> <td>    8.986</td> <td> 0.000</td> <td> 6307.607</td> <td> 9832.187</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>613.565</td> <th>  Durbin-Watson:     </th> <td>   1.911</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>6986.455</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td> 2.656</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td>14.809</td>  <th>  Cond. No.          </th> <td>    30.1</td>\n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &     hhincome     & \\textbf{  R-squared:         } &     0.111   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.108   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     41.45   \\\\\n\\textbf{Date:}             & Wed, 04 Dec 2024 & \\textbf{  Prob (F-statistic):} &  3.03e-25   \\\\\n\\textbf{Time:}             &     23:43:30     & \\textbf{  Log-Likelihood:    } &   -12393.   \\\\\n\\textbf{No. Observations:} &        1000      & \\textbf{  AIC:               } & 2.479e+04   \\\\\n\\textbf{Df Residuals:}     &         996      & \\textbf{  BIC:               } & 2.481e+04   \\\\\n\\textbf{Df Model:}         &           3      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept} &    1991.4203  &     7049.300     &     0.282  &         0.778        &    -1.18e+04    &     1.58e+04     \\\\\n\\textbf{married}   &    2.275e+04  &     3971.744     &     5.729  &         0.000        &      1.5e+04    &     3.05e+04     \\\\\n\\textbf{nchlt5}    &   -6229.1414  &     4232.843     &    -1.472  &         0.141        &    -1.45e+04    &     2077.172     \\\\\n\\textbf{educ}      &    8069.8972  &      898.051     &     8.986  &         0.000        &     6307.607    &     9832.187     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 613.565 & \\textbf{  Durbin-Watson:     } &    1.911  \\\\\n\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 6986.455  \\\\\n\\textbf{Skew:}          &   2.656 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n\\textbf{Kurtosis:}      &  14.809 & \\textbf{  Cond. No.          } &     30.1  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now lets say that data 1 was our original data and we want to make predicitions on data 2\n",
        "\n",
        "x2 = pt.build_design_matrices([x.design_info], data2) #build our nex x based on the old one"
      ],
      "metadata": {
        "id": "JJMsHFlvAfpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6so1hwRA1nY",
        "outputId": "caf39172-617f-4976-99d3-5d55ed52c239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DesignMatrix with shape (1000, 4)\n",
              "  Intercept  married  nchlt5  educ\n",
              "          1        0       0    10\n",
              "          1        0       0     6\n",
              "          1        0       0    10\n",
              "          1        1       0     8\n",
              "          1        1       0     6\n",
              "          1        1       0     6\n",
              "          1        0       0     6\n",
              "          1        1       0     8\n",
              "          1        1       0     6\n",
              "          1        0       0     6\n",
              "          1        1       0    10\n",
              "          1        1       1    11\n",
              "          1        1       0     6\n",
              "          1        0       0    10\n",
              "          1        0       0     6\n",
              "          1        1       0     3\n",
              "          1        0       0     6\n",
              "          1        1       1     8\n",
              "          1        1       0     7\n",
              "          1        0       0     6\n",
              "          1        0       0     6\n",
              "          1        0       0    10\n",
              "          1        0       0    10\n",
              "          1        1       0     6\n",
              "          1        0       1    10\n",
              "          1        0       0    10\n",
              "          1        0       0     7\n",
              "          1        0       0     8\n",
              "          1        1       0     6\n",
              "          1        0       0     6\n",
              "  [970 rows omitted]\n",
              "  Terms:\n",
              "    'Intercept' (column 0)\n",
              "    'married' (column 1)\n",
              "    'nchlt5' (column 2)\n",
              "    'educ' (column 3)\n",
              "  (to view full data, use np.asarray(this_obj))"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x2\n",
        " #now x2 is also 1000 rowsa and 4 colums like x and we didnt need a regression equation to build as it just took the structure and process as x\n",
        "#can make predictions even as new data rolls in\n",
        "\n",
        "\n",
        "#model.predict(x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq-IdmuPA4b6",
        "outputId": "553ecca1-c612-4506-a607-148299572a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DesignMatrix with shape (1000, 4)\n",
              "   Intercept  married  nchlt5  educ\n",
              "           1        1       2     7\n",
              "           1        0       0     6\n",
              "           1        1       0     6\n",
              "           1        1       0     6\n",
              "           1        1       0     6\n",
              "           1        1       0     7\n",
              "           1        1       0    10\n",
              "           1        1       0     6\n",
              "           1        0       0     6\n",
              "           1        1       0     7\n",
              "           1        1       0    10\n",
              "           1        1       1     8\n",
              "           1        1       0     6\n",
              "           1        1       0    10\n",
              "           1        0       0    11\n",
              "           1        0       0     6\n",
              "           1        0       0     4\n",
              "           1        1       0    10\n",
              "           1        1       0     6\n",
              "           1        1       0     6\n",
              "           1        0       0    10\n",
              "           1        0       0    10\n",
              "           1        1       0     6\n",
              "           1        1       0    11\n",
              "           1        0       0     7\n",
              "           1        0       0     7\n",
              "           1        1       0     6\n",
              "           1        1       4     7\n",
              "           1        1       1     2\n",
              "           1        0       0     6\n",
              "   [970 rows omitted]\n",
              "   Terms:\n",
              "     'Intercept' (column 0)\n",
              "     'married' (column 1)\n",
              "     'nchlt5' (column 2)\n",
              "     'educ' (column 3)\n",
              "   (to view full data, use np.asarray(this_obj))]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(x2) #this gives us predications for x2 even though we didnt train it on x2, however now that it is the same shape as x1, you get\n",
        "#reasonable predictions on x2 becaue it structed the same as x\n",
        "\n",
        "#this is why patsy is good, yay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZuiQ5c6DtIS",
        "outputId": "ea8cad75-3adc-4ca8-e33b-aa57390f1d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 68777.2533239 ,  50410.80381352,  73165.63891515,\n",
              "         73165.63891515,  73165.63891515,  81235.53616496,\n",
              "        105445.22791438,  73165.63891515,  50410.80381352,\n",
              "         81235.53616496, 105445.22791438,  83076.29199424,\n",
              "         73165.63891515, 105445.22791438,  90760.29006256,\n",
              "         50410.80381352,  34271.00931391, 105445.22791438,\n",
              "         73165.63891515,  73165.63891515,  82690.39281275,\n",
              "         82690.39281275,  73165.63891515, 113515.12516419,\n",
              "         58480.70106333,  58480.70106333,  73165.63891515,\n",
              "         56318.97048284,  34656.9084954 ,  50410.80381352,\n",
              "         58480.70106333,  73165.63891515, 113515.12516419,\n",
              "        105445.22791438,  73165.63891515, 113515.12516419,\n",
              "         73165.63891515,  89305.43341477,  32816.15266612,\n",
              "         73165.63891515,  50410.80381352, 105445.22791438,\n",
              "         18131.21481429,  89305.43341477,  73165.63891515,\n",
              "         90760.29006256,  92986.94507332,  82690.39281275,\n",
              "         48955.94716573, 105445.22791438,  42340.90656372,\n",
              "        105445.22791438,  89305.43341477,  58480.70106333,\n",
              "         99216.08649385, 113515.12516419, 105445.22791438,\n",
              "         50410.80381352, 105445.22791438,  73165.63891515,\n",
              "         73165.63891515,  82690.39281275,  89305.43341477,\n",
              "         89305.43341477,  73165.63891515,  58480.70106333,\n",
              "         81235.53616496, 105445.22791438,  50410.80381352,\n",
              "         82690.39281275,  73165.63891515,  81235.53616496,\n",
              "         73165.63891515, 101056.84232313,  73165.63891515,\n",
              "         89305.43341477,  58480.70106333,  90760.29006256,\n",
              "        105445.22791438,  81235.53616496, 105445.22791438,\n",
              "         73165.63891515, 105445.22791438,  65095.74166535,\n",
              "         50410.80381352,  50410.80381352,  73165.63891515,\n",
              "         81235.53616496,  50410.80381352, 105445.22791438,\n",
              "         50410.80381352, 113515.12516419,  73165.63891515,\n",
              "        113515.12516419, 105445.22791438, 105445.22791438,\n",
              "         82690.39281275,  50410.80381352,  58480.70106333,\n",
              "         82690.39281275, 105445.22791438, 113515.12516419,\n",
              "         42340.90656372,  58480.70106333,  73165.63891515,\n",
              "        101056.84232313,  73165.63891515, 113515.12516419,\n",
              "         73165.63891515,  92986.94507332,  48955.94716573,\n",
              "        113515.12516419,  50410.80381352, 105445.22791438,\n",
              "        105445.22791438,  81235.53616496,  50410.80381352,\n",
              "         92986.94507332,  81235.53616496,  66550.59831314,\n",
              "         58480.70106333,  50410.80381352, 113515.12516419,\n",
              "         58480.70106333,  81235.53616496,  73165.63891515,\n",
              "         73165.63891515,  73165.63891515,  81235.53616496,\n",
              "         73165.63891515,  50410.80381352,  50410.80381352,\n",
              "         73165.63891515, 113515.12516419,  50410.80381352,\n",
              "         73165.63891515,  83076.29199424,  60707.3560741 ,\n",
              "         73165.63891515,  50410.80381352,  89305.43341477,\n",
              "        105445.22791438,  73165.63891515,  48955.94716573,\n",
              "         22198.62565434,  81235.53616496, 105445.22791438,\n",
              "         73165.63891515,  82690.39281275,  73165.63891515,\n",
              "         73165.63891515,  82690.39281275,  50410.80381352,\n",
              "         66936.49749462,  58480.70106333,  82690.39281275,\n",
              "         73165.63891515, 105445.22791438,  73165.63891515,\n",
              "         73165.63891515,  73165.63891515,  73165.63891515,\n",
              "         73165.63891515, 105445.22791438,  89305.43341477,\n",
              "         73165.63891515,  50410.80381352,  73165.63891515,\n",
              "         89305.43341477,  73165.63891515,  89305.43341477,\n",
              "         50410.80381352, 105445.22791438, 113515.12516419,\n",
              "         81235.53616496,  58480.70106333, 113515.12516419,\n",
              "         65095.74166535,  81235.53616496, 105445.22791438,\n",
              "         58480.70106333, 107285.98374366,  82690.39281275,\n",
              "         82690.39281275,  90760.29006256, 105445.22791438,\n",
              "         73165.63891515,  76847.15057371, 105445.22791438,\n",
              "         89305.43341477, 113515.12516419,  50410.80381352,\n",
              "         50410.80381352,  50410.80381352,  89305.43341477,\n",
              "         57025.84441554,  73165.63891515,  50410.80381352,\n",
              "        105445.22791438,  50410.80381352,  73165.63891515,\n",
              "         81235.53616496, 105445.22791438,  73165.63891515,\n",
              "         90760.29006256,  50410.80381352,  50410.80381352,\n",
              "        113515.12516419,  66550.59831314,  50410.80381352,\n",
              "         82690.39281275,  50410.80381352,  89305.43341477,\n",
              "         66550.59831314,  89305.43341477,  89305.43341477,\n",
              "         50410.80381352,  73165.63891515,  50410.80381352,\n",
              "         89305.43341477,  75006.39474443,  73165.63891515,\n",
              "         58480.70106333,  73165.63891515,  58480.70106333,\n",
              "         73165.63891515, 113515.12516419,  50410.80381352,\n",
              "         66550.59831314,  50410.80381352,  66936.49749462,\n",
              "         81235.53616496,  50410.80381352,  73165.63891515,\n",
              "         73165.63891515, 105445.22791438,  50410.80381352,\n",
              "         73165.63891515,  50410.80381352, 105445.22791438,\n",
              "         57025.84441554,  58480.70106333, 105445.22791438,\n",
              "         81235.53616496,  28427.76707487, 105445.22791438,\n",
              "         73165.63891515, 113515.12516419,  40886.04991593,\n",
              "        105445.22791438,  73165.63891515,  81235.53616496,\n",
              "        105445.22791438,  73165.63891515, 105445.22791438,\n",
              "        105445.22791438,  73165.63891515,  58480.70106333,\n",
              "         50410.80381352, 105445.22791438,  73165.63891515,\n",
              "         73165.63891515,  73165.63891515,  73165.63891515,\n",
              "         73165.63891515,  50410.80381352,  73165.63891515,\n",
              "         50410.80381352, 107285.98374366,  82690.39281275,\n",
              "        113515.12516419,  66550.59831314,  90760.29006256,\n",
              "         73165.63891515,  76847.15057371,  73165.63891515,\n",
              "         66550.59831314,  90760.29006256, 105445.22791438,\n",
              "         73165.63891515,  73165.63891515,  50410.80381352,\n",
              "         73165.63891515,  89305.43341477,  89305.43341477,\n",
              "         58480.70106333,  83076.29199424,  50410.80381352,\n",
              "         81235.53616496, 105445.22791438,  50410.80381352,\n",
              "         50410.80381352, 113515.12516419,  73165.63891515,\n",
              "        105445.22791438,  82690.39281275,  50410.80381352,\n",
              "         73165.63891515, 105445.22791438, 113515.12516419,\n",
              "        105445.22791438,  73165.63891515,  73165.63891515,\n",
              "         73165.63891515,  50410.80381352, 113515.12516419,\n",
              "         50410.80381352,  73165.63891515,  50410.80381352,\n",
              "         50410.80381352, 105445.22791438,  50410.80381352,\n",
              "         50410.80381352,  50410.80381352,  73165.63891515,\n",
              "         89305.43341477,  73165.63891515,  50410.80381352,\n",
              "         99216.08649385,  42340.90656372,  81235.53616496,\n",
              "         50410.80381352,  73165.63891515,  66936.49749462,\n",
              "         50410.80381352,  82690.39281275,  73165.63891515,\n",
              "         50410.80381352,  81235.53616496,  18131.21481429,\n",
              "         81235.53616496,  90760.29006256, 113515.12516419,\n",
              "        105445.22791438,  73165.63891515,  44181.66239299,\n",
              "         32816.15266612, 113515.12516419, 105445.22791438,\n",
              "         50410.80381352,  58480.70106333,  50410.80381352,\n",
              "         81235.53616496,  81235.53616496,  81235.53616496,\n",
              "        105445.22791438,  76847.15057371,  34271.00931391,\n",
              "         50410.80381352,  50410.80381352,  73165.63891515,\n",
              "         73165.63891515,  73165.63891515,  57025.84441554,\n",
              "         66550.59831314,  89305.43341477,  58480.70106333,\n",
              "         73165.63891515,  81235.53616496, 105445.22791438,\n",
              "         73165.63891515,  73165.63891515,  73165.63891515,\n",
              "        107285.98374366,  81235.53616496,  40886.04991593,\n",
              "         73165.63891515,  60707.3560741 ,  50410.80381352,\n",
              "         89305.43341477,  50410.80381352,  73165.63891515,\n",
              "         73165.63891515, 113515.12516419,  73165.63891515,\n",
              "         82690.39281275,  92986.94507332,  40886.04991593,\n",
              "         83076.29199424, 105445.22791438,  81235.53616496,\n",
              "         73165.63891515,  66550.59831314,  73165.63891515,\n",
              "        105445.22791438,  89305.43341477,  50410.80381352,\n",
              "         73165.63891515, 113515.12516419,  73165.63891515,\n",
              "        105445.22791438,  81235.53616496,  58480.70106333,\n",
              "         50410.80381352,  50410.80381352,  81235.53616496,\n",
              "         73165.63891515,  82690.39281275, 105445.22791438,\n",
              "         73165.63891515,  50410.80381352,  58480.70106333,\n",
              "         50410.80381352,  50410.80381352,  58480.70106333,\n",
              "         92986.94507332,  82690.39281275,  58480.70106333,\n",
              "         82690.39281275,  50410.80381352,  73165.63891515,\n",
              "        105445.22791438,  73165.63891515,  50410.80381352,\n",
              "         66936.49749462,  89305.43341477,  60707.3560741 ,\n",
              "        113515.12516419,  73165.63891515,  50410.80381352,\n",
              "         34271.00931391, 105445.22791438,  40886.04991593,\n",
              "         82690.39281275,  40886.04991593,  81235.53616496,\n",
              "         50410.80381352,  73165.63891515, 105445.22791438,\n",
              "         76847.15057371,  99216.08649385,  89305.43341477,\n",
              "         66550.59831314,  90760.29006256,  73165.63891515,\n",
              "         50410.80381352,  82690.39281275,  26201.1120641 ,\n",
              "         99216.08649385,  81235.53616496, 101056.84232313,\n",
              "         90760.29006256,  73165.63891515,  50410.80381352,\n",
              "         50410.80381352,  73165.63891515,  50410.80381352,\n",
              "         75006.39474443,  73165.63891515,  50410.80381352,\n",
              "        113515.12516419,  73165.63891515,  82690.39281275,\n",
              "         73165.63891515,  40886.04991593,  50410.80381352,\n",
              "         92986.94507332,  92986.94507332,  50410.80381352,\n",
              "        105445.22791438,  40886.04991593,  50410.80381352,\n",
              "         81235.53616496, 105445.22791438, 105445.22791438,\n",
              "         58480.70106333,  50410.80381352,  99216.08649385,\n",
              "         75006.39474443,  73165.63891515,  76461.25139222,\n",
              "         58480.70106333,  73165.63891515, 105445.22791438,\n",
              "         58480.70106333, 113515.12516419,  81235.53616496,\n",
              "         89305.43341477,  99216.08649385,  50410.80381352,\n",
              "        113515.12516419,  89305.43341477,  50410.80381352,\n",
              "         75006.39474443, 105445.22791438,  34271.00931391,\n",
              "         82690.39281275,  73165.63891515,  73165.63891515,\n",
              "         58480.70106333,  58480.70106333,  73165.63891515,\n",
              "         66550.59831314,  50410.80381352,  81235.53616496,\n",
              "         58480.70106333,  73165.63891515, 105445.22791438,\n",
              "        105445.22791438,  82690.39281275,  73165.63891515,\n",
              "         58480.70106333,  58480.70106333,  73165.63891515,\n",
              "        105445.22791438,  82690.39281275,  50410.80381352,\n",
              "         73165.63891515, 105445.22791438,  50410.80381352,\n",
              "         89305.43341477,  73165.63891515,  82690.39281275,\n",
              "         82690.39281275,  42340.90656372,  73165.63891515,\n",
              "         50410.80381352,  81235.53616496, 105445.22791438,\n",
              "         73165.63891515,  73165.63891515,  82690.39281275,\n",
              "         58480.70106333, 105445.22791438,  81235.53616496,\n",
              "         83076.29199424,  81235.53616496,  73165.63891515,\n",
              "         50410.80381352,  73165.63891515,  73165.63891515,\n",
              "         42340.90656372,  58480.70106333,  50410.80381352,\n",
              "         73165.63891515,  73165.63891515,  76847.15057371,\n",
              "         50410.80381352,  89305.43341477,  73165.63891515,\n",
              "         83076.29199424,  52251.5596428 ,  73165.63891515,\n",
              "        113515.12516419,  82690.39281275,  73165.63891515,\n",
              "         82690.39281275,  81235.53616496,  73165.63891515,\n",
              "        105445.22791438,  92986.94507332,  81235.53616496,\n",
              "         50410.80381352,  50410.80381352,  58480.70106333,\n",
              "         82690.39281275,  50410.80381352,  40886.04991593,\n",
              "        105445.22791438,  58480.70106333,  73165.63891515,\n",
              "         58480.70106333, 113515.12516419, 105445.22791438,\n",
              "        105445.22791438,  73165.63891515,  81235.53616496,\n",
              "         89305.43341477,  73165.63891515,  60707.3560741 ,\n",
              "        107285.98374366,  73165.63891515,  50410.80381352,\n",
              "         82690.39281275, 105445.22791438,  32816.15266612,\n",
              "        105445.22791438,  50410.80381352,  73165.63891515,\n",
              "         50410.80381352,  82690.39281275,  73165.63891515,\n",
              "         58480.70106333,  42340.90656372,  50410.80381352,\n",
              "         73165.63891515,  58480.70106333,  81235.53616496,\n",
              "         32816.15266612,  50410.80381352,  66550.59831314,\n",
              "         89305.43341477, 113515.12516419,  99216.08649385,\n",
              "         65095.74166535,  73165.63891515,  99216.08649385,\n",
              "         73165.63891515, 105445.22791438, 113515.12516419,\n",
              "         66550.59831314, 105445.22791438,  81235.53616496,\n",
              "         58480.70106333,  50410.80381352,  58480.70106333,\n",
              "         75006.39474443, 105445.22791438,  89305.43341477,\n",
              "         50410.80381352,  73165.63891515, 113515.12516419,\n",
              "         50410.80381352,  82690.39281275,  50410.80381352,\n",
              "         50410.80381352,  73165.63891515, 113515.12516419,\n",
              "        105445.22791438, 107285.98374366,  73165.63891515,\n",
              "         89305.43341477,  76847.15057371,  73165.63891515,\n",
              "         50410.80381352,  32816.15266612,  50410.80381352,\n",
              "         58480.70106333,  81235.53616496,  58480.70106333,\n",
              "         73165.63891515,  58480.70106333, 105445.22791438,\n",
              "        105445.22791438,  81235.53616496, 113515.12516419,\n",
              "         73165.63891515,  66936.49749462,  18131.21481429,\n",
              "        113515.12516419,  89305.43341477,  50410.80381352,\n",
              "        113515.12516419, 105445.22791438,  94827.7009026 ,\n",
              "         81235.53616496,  40886.04991593, 113515.12516419,\n",
              "         73165.63891515,  99216.08649385,  50410.80381352,\n",
              "        105445.22791438,  66550.59831314,  90760.29006256,\n",
              "         50410.80381352,  50410.80381352, 105445.22791438,\n",
              "         73165.63891515,  73165.63891515,  73165.63891515,\n",
              "         73165.63891515,  73165.63891515,  66550.59831314,\n",
              "        105445.22791438,  50410.80381352, 107285.98374366,\n",
              "         50410.80381352,  58480.70106333,  50410.80381352,\n",
              "        113515.12516419,  73165.63891515,  50410.80381352,\n",
              "         50410.80381352,  82690.39281275,  73165.63891515,\n",
              "         81235.53616496,  50410.80381352,  73165.63891515,\n",
              "         73165.63891515,  50410.80381352,  73165.63891515,\n",
              "         73165.63891515, 105445.22791438,  99216.08649385,\n",
              "         82690.39281275,  50796.70299501,  81235.53616496,\n",
              "        107285.98374366,  99216.08649385,  50410.80381352,\n",
              "         89305.43341477,  89305.43341477,  50410.80381352,\n",
              "         66550.59831314,  89305.43341477,  48955.94716573,\n",
              "         68777.2533239 , 105445.22791438,  58480.70106333,\n",
              "        105445.22791438,  73165.63891515,  58480.70106333,\n",
              "         81235.53616496,  92986.94507332,  82690.39281275,\n",
              "         83076.29199424,  50410.80381352,  81235.53616496,\n",
              "         81235.53616496,  81235.53616496, 113515.12516419,\n",
              "        105445.22791438,  66550.59831314, 105445.22791438,\n",
              "         89305.43341477,  73165.63891515, 113515.12516419,\n",
              "         73165.63891515,  18131.21481429, 105445.22791438,\n",
              "         73165.63891515,  81235.53616496,  66550.59831314,\n",
              "         89305.43341477,  75006.39474443,  89305.43341477,\n",
              "         50410.80381352,  65095.74166535,  82690.39281275,\n",
              "         73165.63891515,  82690.39281275,  82690.39281275,\n",
              "         50410.80381352, 105445.22791438,  73165.63891515,\n",
              "        107285.98374366,  81235.53616496, 105445.22791438,\n",
              "         50410.80381352,  58480.70106333,  89305.43341477,\n",
              "         44181.66239299,  58480.70106333,  34271.00931391,\n",
              "         73165.63891515, 107285.98374366,  58480.70106333,\n",
              "        101056.84232313, 113515.12516419,  73165.63891515,\n",
              "         50410.80381352, 113515.12516419,  89305.43341477,\n",
              "         73165.63891515,  50410.80381352,  73165.63891515,\n",
              "         58480.70106333,  50410.80381352, 105445.22791438,\n",
              "         50410.80381352,  73165.63891515,  82690.39281275,\n",
              "        105445.22791438,  50410.80381352,  73165.63891515,\n",
              "         89305.43341477,  73165.63891515, 105445.22791438,\n",
              "         89305.43341477, 105445.22791438,  32816.15266612,\n",
              "         50410.80381352,  99216.08649385,  73165.63891515,\n",
              "         50410.80381352,  82690.39281275,  82690.39281275,\n",
              "         50410.80381352,  66936.49749462, 105445.22791438,\n",
              "        105445.22791438,  82690.39281275,  73165.63891515,\n",
              "         92986.94507332,  73165.63891515,  57025.84441554,\n",
              "         50410.80381352, 105445.22791438,  73165.63891515,\n",
              "         58480.70106333,  73165.63891515,  66550.59831314,\n",
              "         89305.43341477,  73165.63891515,  73165.63891515,\n",
              "         42340.90656372,  58480.70106333,  81235.53616496,\n",
              "         73165.63891515,  90760.29006256,  83076.29199424,\n",
              "         73165.63891515, 105445.22791438,  73165.63891515,\n",
              "         50410.80381352, 113515.12516419,  44181.66239299,\n",
              "         50410.80381352, 113515.12516419,  58480.70106333,\n",
              "         66550.59831314,  73165.63891515,  92986.94507332,\n",
              "         81235.53616496,  50410.80381352,  73165.63891515,\n",
              "         50410.80381352,  50410.80381352,  50410.80381352,\n",
              "         50410.80381352,  73165.63891515,  89305.43341477,\n",
              "         73165.63891515,  82690.39281275,  65095.74166535,\n",
              "         50410.80381352,  66936.49749462, 113515.12516419,\n",
              "         81235.53616496,  73165.63891515, 105445.22791438,\n",
              "         82690.39281275,  81235.53616496, 113515.12516419,\n",
              "        105445.22791438,  82690.39281275, 113515.12516419,\n",
              "         50410.80381352,  99216.08649385,  58480.70106333,\n",
              "        113515.12516419,  90760.29006256,  82690.39281275,\n",
              "         26201.1120641 ,  73165.63891515,  73165.63891515,\n",
              "         76847.15057371,  50410.80381352,  89305.43341477,\n",
              "         89305.43341477,  50410.80381352,  89305.43341477,\n",
              "         58480.70106333,  81235.53616496,  50410.80381352,\n",
              "         73165.63891515,  82690.39281275,  73165.63891515,\n",
              "         32816.15266612,  73165.63891515,  73165.63891515,\n",
              "         73165.63891515, 105445.22791438,  58480.70106333,\n",
              "        107285.98374366,  81235.53616496,  50410.80381352,\n",
              "         50410.80381352, 105445.22791438, 105445.22791438,\n",
              "         58480.70106333, 113515.12516419,  89305.43341477,\n",
              "         81235.53616496,  92986.94507332, 105445.22791438,\n",
              "         82690.39281275,  73165.63891515,  73165.63891515,\n",
              "         73165.63891515,  66550.59831314,  82690.39281275,\n",
              "        107285.98374366,  50410.80381352,  73165.63891515,\n",
              "         82690.39281275,  50410.80381352,  76847.15057371,\n",
              "         73165.63891515, 113515.12516419,  50410.80381352,\n",
              "         89305.43341477, 113515.12516419,  81235.53616496,\n",
              "         89305.43341477,  99216.08649385,  50410.80381352,\n",
              "        105445.22791438, 113515.12516419,  73165.63891515,\n",
              "         73165.63891515,  89305.43341477,  50410.80381352,\n",
              "         73165.63891515, 113515.12516419, 113515.12516419,\n",
              "         73165.63891515,  82690.39281275,  73165.63891515,\n",
              "         66550.59831314,  89305.43341477,  81235.53616496,\n",
              "         73165.63891515, 113515.12516419,  75006.39474443,\n",
              "        105445.22791438,  81235.53616496,  73165.63891515,\n",
              "         50410.80381352,  58480.70106333, 113515.12516419,\n",
              "        105445.22791438,  60707.3560741 ,  82690.39281275,\n",
              "         73165.63891515,  73165.63891515, 105445.22791438,\n",
              "         50410.80381352,  58480.70106333,  81235.53616496,\n",
              "         58480.70106333, 113515.12516419,  66936.49749462,\n",
              "         73165.63891515,  73165.63891515, 113515.12516419,\n",
              "        105445.22791438,  50410.80381352,  81235.53616496,\n",
              "         54478.21465357,  50410.80381352,  73165.63891515,\n",
              "        105445.22791438,  50410.80381352, 101056.84232313,\n",
              "         73165.63891515,  50410.80381352,  65095.74166535,\n",
              "         65095.74166535,  89305.43341477,  50410.80381352,\n",
              "         82690.39281275,  82690.39281275,  73165.63891515,\n",
              "         73165.63891515,  48955.94716573,  50410.80381352,\n",
              "        105445.22791438, 113515.12516419,  58480.70106333,\n",
              "         76847.15057371,  82690.39281275,  73165.63891515,\n",
              "         73165.63891515, 113515.12516419,  89305.43341477,\n",
              "         73165.63891515,  81235.53616496,  73165.63891515,\n",
              "         82690.39281275,  42340.90656372,  48955.94716573,\n",
              "         58480.70106333,  50410.80381352,  73165.63891515,\n",
              "         58480.70106333,  58480.70106333,  73165.63891515,\n",
              "         89305.43341477,  73165.63891515,  73165.63891515,\n",
              "         81235.53616496, 113515.12516419,  73165.63891515,\n",
              "         82690.39281275,  50410.80381352, 105445.22791438,\n",
              "         50410.80381352, 105445.22791438,  50410.80381352,\n",
              "         81235.53616496, 113515.12516419,  50410.80381352,\n",
              "         73165.63891515,  73165.63891515,  58480.70106333,\n",
              "         50410.80381352,  73165.63891515,  73165.63891515,\n",
              "         73165.63891515]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MACHINE LEARNING"
      ],
      "metadata": {
        "id": "4e3aevGJEIcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scikit-learn in the library most used in pythin for machine learn\n",
        "\n",
        "#predictive modeling\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import patsy as pt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Import some data...\n",
        "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/raw/master/DataSets/occupancyTrain.csv\")\n",
        "\n",
        "\n",
        "# Build x, y matrices\n",
        "y, x = pt.dmatrices(\"Occupancy ~ -1 + Light + CO2\", data=data) #the -1 in patsy means no intercept column, most machine learning models dont like intercept terms\n",
        "\n"
      ],
      "metadata": {
        "id": "t4kS1A7FEHgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y # represents 1 and 0 to tells us when the room is or isnt occupie"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oCAvfXUEuli",
        "outputId": "fd87cd9e-53f8-4c41-dcde-9855ae4b874d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DesignMatrix with shape (8143, 1)\n",
              "  Occupancy\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          1\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "          0\n",
              "  [8113 rows omitted]\n",
              "  Terms:\n",
              "    'Occupancy' (column 0)\n",
              "  (to view full data, use np.asarray(this_obj))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x #notice not intercept column on the 2 variables, tryign to use these to predict room occupancy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4a0KSsWE8iF",
        "outputId": "8ebc468b-acea-4daf-aa72-561866c4ddb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DesignMatrix with shape (8143, 2)\n",
              "  Light        CO2\n",
              "  426.0  721.25000\n",
              "  429.5  714.00000\n",
              "  426.0  713.50000\n",
              "  426.0  708.25000\n",
              "  426.0  704.50000\n",
              "  419.0  701.00000\n",
              "  419.0  701.66667\n",
              "  419.0  699.00000\n",
              "  419.0  689.33333\n",
              "  419.0  688.00000\n",
              "  419.0  690.25000\n",
              "  419.0  691.00000\n",
              "  419.0  683.50000\n",
              "  419.0  687.50000\n",
              "  419.0  686.00000\n",
              "  418.5  680.50000\n",
              "    0.0  681.50000\n",
              "    0.0  685.00000\n",
              "    0.0  685.00000\n",
              "    0.0  689.00000\n",
              "    0.0  689.50000\n",
              "    0.0  689.00000\n",
              "    0.0  691.00000\n",
              "    0.0  688.00000\n",
              "    0.0  689.50000\n",
              "    0.0  689.00000\n",
              "    0.0  685.66667\n",
              "    0.0  687.00000\n",
              "    0.0  688.00000\n",
              "    0.0  670.00000\n",
              "  [8113 rows omitted]\n",
              "  Terms:\n",
              "    'Light' (column 0)\n",
              "    'CO2' (column 1)\n",
              "  (to view full data, use np.asarray(this_obj))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#will be doing classificiation trees\n",
        "\n",
        "from sklearn import tree #imports all tree based code\n",
        "from sklearn.metrics import accuracy_score #accuracy score is what well use to test performance of predictive model (% of observations correctly claisifed)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x1, x2, y1, y2 = train_test_split(x,y) #split x and y varables, shuffle them, and randomly assign observations to a training and testing data set\n",
        "\n",
        "#we have a binary classification problem (either the room is or isnt claissified ) so accuracy score is going to be the % of observations we corectly classify if the\n",
        "#room is occupioed\n",
        "\n",
        "clf = tree.DecisionTreeClassifier() #this creates classifier by initiating an instance of the classifier object, now clf is classifier object\n",
        "\n",
        "#now need to modify the decision tree classifier object. When using sklearn we put x and y into the fit function and x comes before y\n",
        "clf = clf.fit(x1, y1) #fit the classifier\n",
        "\n",
        "#make predictions ased on the data we have\n",
        "pred = clf.predict(x2) #predict based on x2\n",
        "\n",
        "print(accuracy_score(y2, pred)) #measure accuray score based on y2v <-here we are comparing the predictions in pred with x2 to the true variables which come from y2\n",
        "#this should give us an acuracy score\n",
        "\n",
        "#the score means that based on co2 and light we are going to be about 98% accurate in determing is a room is occupied  (tells us how well our mode performed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8Vw9RbXFE2m",
        "outputId": "acec2959-6356-448a-eff0-aaf693925fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.987721021611002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P2FKoy_HoBx",
        "outputId": "fe9d2cc0-9220-4960-b857-43b7ea4aa8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0.        , 1392.33333333],\n",
              "       [   0.        ,  441.5       ],\n",
              "       [ 405.        ,  655.5       ],\n",
              "       ...,\n",
              "       [ 454.        , 1338.        ],\n",
              "       [   0.        ,  432.5       ],\n",
              "       [   0.        ,  433.5       ]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now we can do a support vector machine\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = svm.SVC()\n",
        "clf = clf.fit(x1, y1)\n",
        "\n",
        "pred = clf.predict(x2)\n",
        "\n",
        "print(accuracy_score(y2, pred))\n",
        "\n",
        "#This one gave us a highrt accuracy score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuZ8YnluIWFK",
        "outputId": "514ef683-d2a1-46ea-8402-9736dfb18e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9891944990176817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now try a random forest model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators = 100) #do have to provide one argument for the number of \"trees\" in our \"fores\"\n",
        "clf = clf.fit(x1, y1)\n",
        "\n",
        "pred = clf.predict(x2)\n",
        "\n",
        "print(accuracy_score(y2, pred))\n",
        "\n",
        "#this model does a little better than the other ones"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOT_6NTOIywU",
        "outputId": "838be7f5-7707-4ee9-e293-b6f5d8b8d1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9896856581532416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now we can go back and look at our predictions to see which observations are successes and failures and go from there\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdpUdxX2JVsf",
        "outputId": "16ea6511-8237-47c2-923a-38a4b4fe8357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}